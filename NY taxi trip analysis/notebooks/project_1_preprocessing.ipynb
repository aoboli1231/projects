{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b7e5cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark to read parquet files\n",
    "from pyspark.sql import SparkSession\n",
    "# start a spark session, preprocessing\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Project1 preprocessing\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.debug.maxToStringFields\", \"100000\")\n",
    "    .config(\"spark.executor.memory\", \"8g\")\n",
    "    .config(\"spark.driver.memory\", \"8g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "# import functions in pyspark for date filtering\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d882d0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib #used to create new folder to store filtered dataset\n",
    "\n",
    "# set up working directories for Yellow Taxi and High Volume For-Hire Vehicle trip records from 2019.2 \n",
    "#to 2019.8\n",
    "\n",
    "\n",
    "########################################################################\n",
    "\n",
    "\n",
    "# NOTE, Need to replace the working directions of the file\n",
    "\n",
    "# please do not include back slash at the end\n",
    "\n",
    "# remove current directory and change it to your directory to the project\n",
    "\n",
    "\n",
    "directory = \"/Users/aobo/Desktop/project_1\"\n",
    "\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ytaxi_raw_train = directory+'/data/raw/ytaxi/train/'\n",
    "HVFH_raw_train = directory+'/data/raw/HVFH/train/'\n",
    "Ytaxi_raw_test = directory+'/data/raw/ytaxi/test/'\n",
    "HVFH_raw_test = directory+'/data/raw/HVFH/test/'\n",
    "\n",
    "# obtain HVFH and Yellow Taxi's file name to filter each file with that specific month's trip data\n",
    "# can use os to obtain all file names in the folder, however since names are little, typed names is\n",
    "# is more efficient\n",
    "\n",
    "ytaxi_name = [\"0219ytaxi.parquet\", \"0319ytaxi.parquet\", \"0419ytaxi.parquet\", \"0519ytaxi.parquet\", \n",
    "              \"0619ytaxi.parquet\", \"0719ytaxi.parquet\", \"0819ytaxi.parquet\", \"0919ytaxi.parquet\"]\n",
    "HVFH_name = [\"0219HVFH.parquet\", \"0319HVFH.parquet\", \"0419HVFH.parquet\", \"0519HVFH.parquet\",\n",
    "             \"0619HVFH.parquet\", \"0719HVFH.parquet\", \"0819HVFH.parquet\", \"0919HVFH.parquet\"]\n",
    "\n",
    "\n",
    "\n",
    "# create new folder for different taxi type\n",
    "# seperate data into training and testing \n",
    "pathlib.Path(directory+\"/data/curated/HVFH/train/\").mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path(directory+\"/data/curated/ytaxi/train/\").mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path(directory+\"/data/curated/HVFH/test/\").mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path(directory+\"/data/curated/ytaxi/test/\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create filtered dataset's working directory\n",
    "filter_ytaxi_train = directory+\"/data/curated/ytaxi/train/\"\n",
    "filter_HVFH_train = directory+\"/data/curated/HVFH/train/\"\n",
    "filter_ytaxi_test = directory+\"/data/curated/ytaxi/test/\"\n",
    "filter_HVFH_test = directory+\"/data/curated/HVFH/test/\"\n",
    "\n",
    "# time list, each attribute indicate the start of month\n",
    "date = [\"2019-02-01 00:00:00\", \"2019-03-01 00:00:00\", \"2019-04-01 00:00:00\", \"2019-05-01 00:00:00\",\n",
    "        \"2019-06-01 00:00:00\", \"2019-07-01 00:00:00\", \"2019-08-01 00:00:00\", \"2019-09-01 00:00:00\",\n",
    "        \"2019-10-01 00:00:00\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c99b0a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter notebook fails to read the files with folder directory, creating [Errno 61] Connection refused\n",
    "# that needs to restart the kernel, other all other spark.read.parquet fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44432c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read example New York Weather csv file\n",
    "NY_weather_2 = spark.read.parquet(directory+\"/data/raw/weather/parquet/weather_2_2019.parquet\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "demographic-compiler",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Date</th><th>TMid</th><th>Weather</th><th>day</th><th>end_day</th></tr>\n",
       "<tr><td>2019-02-01</td><td>16.5</td><td>3</td><td>0</td><td>2019-02-02 00:00:00</td></tr>\n",
       "<tr><td>2019-02-02</td><td>25.0</td><td>0</td><td>1</td><td>2019-02-03 00:00:00</td></tr>\n",
       "<tr><td>2019-02-03</td><td>39.5</td><td>11</td><td>1</td><td>2019-02-04 00:00:00</td></tr>\n",
       "<tr><td>2019-02-04</td><td>45.0</td><td>13</td><td>0</td><td>2019-02-05 00:00:00</td></tr>\n",
       "<tr><td>2019-02-05</td><td>51.0</td><td>13</td><td>0</td><td>2019-02-06 00:00:00</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+----+-------+---+-------------------+\n",
       "|      Date|TMid|Weather|day|            end_day|\n",
       "+----------+----+-------+---+-------------------+\n",
       "|2019-02-01|16.5|      3|  0|2019-02-02 00:00:00|\n",
       "|2019-02-02|25.0|      0|  1|2019-02-03 00:00:00|\n",
       "|2019-02-03|39.5|     11|  1|2019-02-04 00:00:00|\n",
       "|2019-02-04|45.0|     13|  0|2019-02-05 00:00:00|\n",
       "|2019-02-05|51.0|     13|  0|2019-02-06 00:00:00|\n",
       "+----------+----+-------+---+-------------------+"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if file is correctly read\n",
    "NY_weather_2.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b6adf67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>start_time</th><th>end_time</th><th>event</th></tr>\n",
       "<tr><td>2019-02-01 18:00:00</td><td>2019-02-01 19:30:00</td><td>1</td></tr>\n",
       "<tr><td>2019-02-01 21:30:00</td><td>2019-02-01 23:00:00</td><td>1</td></tr>\n",
       "<tr><td>2019-02-02 18:00:00</td><td>2019-02-02 19:00:00</td><td>1</td></tr>\n",
       "<tr><td>2019-02-02 20:30:00</td><td>2019-02-02 21:30:00</td><td>1</td></tr>\n",
       "<tr><td>2019-02-03 11:00:00</td><td>2019-02-03 14:00:00</td><td>1</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------------------+-------------------+-----+\n",
       "|         start_time|           end_time|event|\n",
       "+-------------------+-------------------+-----+\n",
       "|2019-02-01 18:00:00|2019-02-01 19:30:00|    1|\n",
       "|2019-02-01 21:30:00|2019-02-01 23:00:00|    1|\n",
       "|2019-02-02 18:00:00|2019-02-02 19:00:00|    1|\n",
       "|2019-02-02 20:30:00|2019-02-02 21:30:00|    1|\n",
       "|2019-02-03 11:00:00|2019-02-03 14:00:00|    1|\n",
       "+-------------------+-------------------+-----+"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read sport event in 2019 New York csv, and check\n",
    "sport_2 = spark.read.parquet(directory+\"/data/raw/sport/parquet/final_sports_2_2019.parquet\", header=True)\n",
    "sport_2.limit(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbe50c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are only interested in pickup time, dropoff time, trip distance, passenger count, pick up & \n",
    "# drop off location and amount that is paid\n",
    "\n",
    "HVFH_required_col = [\"pickup_datetime\", \"PULocationID\", \"DOLocationID\", \"trip_miles\",\n",
    "                    \"trip_time\", \"total_amount\"]\n",
    "ytaxi_required_col = [\"tpep_pickup_datetime\", \"passenger_count\", \"trip_distance\",\"RatecodeID\",\n",
    "                      \"PULocationID\", \"DOLocationID\", \"total_amount\", \"trip_Time\"]\n",
    "\n",
    "# HVFH data do not contain passenger count, therefore it will not be included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "composite-mixture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>hvfhs_license_num</th><th>dispatching_base_num</th><th>originating_base_num</th><th>request_datetime</th><th>on_scene_datetime</th><th>pickup_datetime</th><th>dropoff_datetime</th><th>PULocationID</th><th>DOLocationID</th><th>trip_miles</th><th>trip_time</th><th>base_passenger_fare</th><th>tolls</th><th>bcf</th><th>sales_tax</th><th>congestion_surcharge</th><th>airport_fee</th><th>tips</th><th>driver_pay</th><th>shared_request_flag</th><th>shared_match_flag</th><th>access_a_ride_flag</th><th>wav_request_flag</th><th>wav_match_flag</th></tr>\n",
       "<tr><td>HV0003</td><td>B02764</td><td>B02764</td><td>2019-09-01 10:03:39</td><td>2019-09-01 10:08:57</td><td>2019-09-01 10:10:02</td><td>2019-09-01 10:17:07</td><td>136</td><td>169</td><td>1.63</td><td>426</td><td>6.91</td><td>0.0</td><td>0.0</td><td>0.6</td><td>0.0</td><td>null</td><td>0.0</td><td>5.39</td><td>N</td><td>N</td><td> </td><td>N</td><td>N</td></tr>\n",
       "<tr><td>HV0003</td><td>B02764</td><td>B02764</td><td>2019-09-01 10:14:31</td><td>2019-09-01 10:20:53</td><td>2019-09-01 10:22:37</td><td>2019-09-01 10:31:53</td><td>169</td><td>18</td><td>1.6</td><td>556</td><td>8.48</td><td>0.0</td><td>0.0</td><td>0.73</td><td>0.0</td><td>null</td><td>0.0</td><td>6.34</td><td>N</td><td>N</td><td> </td><td>N</td><td>N</td></tr>\n",
       "<tr><td>HV0003</td><td>B02764</td><td>B02764</td><td>2019-09-01 10:29:21</td><td>2019-09-01 10:33:26</td><td>2019-09-01 10:36:03</td><td>2019-09-01 11:04:01</td><td>94</td><td>198</td><td>18.01</td><td>1678</td><td>20.65</td><td>6.12</td><td>0.0</td><td>2.32</td><td>0.0</td><td>null</td><td>0.0</td><td>39.68</td><td>Y</td><td>N</td><td> </td><td>N</td><td>N</td></tr>\n",
       "<tr><td>HV0005</td><td>B02510</td><td>null</td><td>2019-09-01 10:00:12</td><td>null</td><td>2019-09-01 10:04:04</td><td>2019-09-01 10:26:58</td><td>114</td><td>112</td><td>4.834</td><td>1374</td><td>23.13</td><td>2.0</td><td>0.63</td><td>2.23</td><td>2.75</td><td>null</td><td>4.61</td><td>16.64</td><td>N</td><td>N</td><td>N</td><td>N</td><td>N</td></tr>\n",
       "<tr><td>HV0005</td><td>B02510</td><td>null</td><td>2019-09-01 10:27:17</td><td>null</td><td>2019-09-01 10:33:15</td><td>2019-09-01 10:41:07</td><td>112</td><td>112</td><td>1.941</td><td>472</td><td>4.44</td><td>0.0</td><td>0.11</td><td>0.39</td><td>0.0</td><td>null</td><td>0.0</td><td>0.0</td><td>Y</td><td>N</td><td>N</td><td>N</td><td>N</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
       "|hvfhs_license_num|dispatching_base_num|originating_base_num|   request_datetime|  on_scene_datetime|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|trip_miles|trip_time|base_passenger_fare|tolls| bcf|sales_tax|congestion_surcharge|airport_fee|tips|driver_pay|shared_request_flag|shared_match_flag|access_a_ride_flag|wav_request_flag|wav_match_flag|\n",
       "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
       "|           HV0003|              B02764|              B02764|2019-09-01 10:03:39|2019-09-01 10:08:57|2019-09-01 10:10:02|2019-09-01 10:17:07|         136|         169|      1.63|      426|               6.91|  0.0| 0.0|      0.6|                 0.0|       null| 0.0|      5.39|                  N|                N|                  |               N|             N|\n",
       "|           HV0003|              B02764|              B02764|2019-09-01 10:14:31|2019-09-01 10:20:53|2019-09-01 10:22:37|2019-09-01 10:31:53|         169|          18|       1.6|      556|               8.48|  0.0| 0.0|     0.73|                 0.0|       null| 0.0|      6.34|                  N|                N|                  |               N|             N|\n",
       "|           HV0003|              B02764|              B02764|2019-09-01 10:29:21|2019-09-01 10:33:26|2019-09-01 10:36:03|2019-09-01 11:04:01|          94|         198|     18.01|     1678|              20.65| 6.12| 0.0|     2.32|                 0.0|       null| 0.0|     39.68|                  Y|                N|                  |               N|             N|\n",
       "|           HV0005|              B02510|                null|2019-09-01 10:00:12|               null|2019-09-01 10:04:04|2019-09-01 10:26:58|         114|         112|     4.834|     1374|              23.13|  2.0|0.63|     2.23|                2.75|       null|4.61|     16.64|                  N|                N|                 N|               N|             N|\n",
       "|           HV0005|              B02510|                null|2019-09-01 10:27:17|               null|2019-09-01 10:33:15|2019-09-01 10:41:07|         112|         112|     1.941|      472|               4.44|  0.0|0.11|     0.39|                 0.0|       null| 0.0|       0.0|                  Y|                N|                 N|               N|             N|\n",
       "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test read one file\n",
    "test_0919 = spark.read.parquet(directory+\"/data/raw/HVFH/test/0919HVFH.parquet\")\n",
    "test_0919.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "angry-spain",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import date_format\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# this function filters HVFH and Yellow taxi monthly data, join the filtered dataframe with weather and \n",
    "# sports data, and export the new dataframe as parquet file\n",
    "\n",
    "def filter_file(ytaxi_dir, HVFH_dir, num, sport, weather, ytaxi_save_dir, HVFH_save_dir):\n",
    "    \n",
    "    # read raw data parquet file\n",
    "    temp_ytaxi_data = spark.read.parquet(ytaxi_dir+ytaxi_name[num])\n",
    "    temp_HVFH_data = spark.read.parquet(HVFH_dir+HVFH_name[num])\n",
    "    \n",
    "    \n",
    "    # fill null values in airport_fee with 0\n",
    "    temp_HVFH_data = temp_HVFH_data.fillna(int(0),\"airport_fee\")\n",
    "\n",
    "    # add up the total trip cost for HVFH data\n",
    "    temp_HVFH_data = temp_HVFH_data.withColumn(\"total_amount\", \n",
    "                                               round(F.col(\"base_passenger_fare\").cast(\"double\")\n",
    "                                                        +F.col(\"tolls\").cast(\"double\")\n",
    "                                                        +F.col(\"bcf\").cast(\"double\")\n",
    "                                                        +F.col(\"sales_tax\").cast(\"double\")\n",
    "                                                        +F.col(\"airport_fee\").cast(\"double\")\n",
    "                                                        +F.col(\"congestion_surcharge\").cast(\"double\"),3))\n",
    "    \n",
    "    # first create column with if each row has valid record, only obtain uber as HVFH data \n",
    "    # (dataset is too big for each month)\n",
    "    \n",
    "    temp_HVFH_data = temp_HVFH_data.withColumn(\n",
    "        'is_valid_record',\n",
    "        # when we have a positive distance/passenger/total amount then True\n",
    "        # else False\n",
    "        F.when(\n",
    "            # trip distance, trip time total cost should be greater than 0, anything less \n",
    "            # or equal is not valid\n",
    "            \n",
    "            # remove outliers, if trip distance is more than 100 miles, total cost over $300 and trip\n",
    "            # time over 2 hours\n",
    "            (F.col('trip_miles') > 0)\n",
    "            &(F.col('trip_miles') < 100)\n",
    "            &(F.col('total_amount') > 0)\n",
    "            &(F.col('total_amount') <300)\n",
    "            & (F.col('trip_Time') > 0)\n",
    "            & (F.col('trip_Time') < 7200)\n",
    "            \n",
    "            # only select Uber as HVFH data\n",
    "            &(F.col('hvfhs_license_num') == \"HV0003\")\n",
    "            \n",
    "             # filter pickup time, only pickup date data within the month is valid\n",
    "            & (F.col('pickup_datetime') >= date[num])\n",
    "            & (F.col('pickup_datetime') < date[num+1]),\n",
    "            True\n",
    "        ).otherwise(False))\n",
    "        \n",
    "    # calculate trip time for yellow taxi\n",
    "    temp_ytaxi_data = temp_ytaxi_data.withColumn(\"trip_Time\", (F.col(\"tpep_dropoff_datetime\").cast(\"int\")\n",
    "                                                     - F.col(\"tpep_pickup_datetime\").cast(\"int\")))\n",
    "    \n",
    "    temp_ytaxi_data = temp_ytaxi_data.withColumn(\n",
    "        'is_valid_record',\n",
    "        # when we have a positive distance/passenger/total amount then True\n",
    "        # else False\n",
    "        F.when(\n",
    "            # trip distance, trip time total cost should be greater than 0, anything less \n",
    "            # or equal is not valid\n",
    "            \n",
    "            # remove outliers, if trip distance is more than 100 miles, total cost over $300 and trip\n",
    "            # time over 2 hours\n",
    "            (F.col('trip_distance') > 0)\n",
    "            &(F.col('total_amount') > 0)\n",
    "            & (F.col('trip_Time') > 0)\n",
    "            &(F.col('trip_distance') < 100)\n",
    "            &(F.col('total_amount') <300)\n",
    "            & (F.col('trip_Time') < 7200)\n",
    "            \n",
    "            # filter pickup time, only pickup date data within the month is valid\n",
    "            & (F.col('tpep_pickup_datetime') >= date[num])\n",
    "            & (F.col('tpep_pickup_datetime') < date[num+1]),\n",
    "            True\n",
    "        ).otherwise(False))\n",
    "    \n",
    "    # First select valid data only, that is no null values in required columns\n",
    "    temp_ytaxi_data = temp_ytaxi_data.filter(col(\"is_valid_record\")==True)\n",
    "    temp_HVFH_data = temp_HVFH_data.filter(col(\"is_valid_record\")==True)\n",
    "    \n",
    "    \n",
    "    # filter first to reduce data dimension for joining dataframe\n",
    "    temp_ytaxi_data = temp_ytaxi_data.select([*ytaxi_required_col])\n",
    "    temp_HVFH_data = temp_HVFH_data.select([*HVFH_required_col])\n",
    "\n",
    "    \n",
    "    # join trip data with weather and sport data\n",
    "    # join if pickup time is within sport event time, and weather if it's that day\n",
    "    temp_ytaxi_data = temp_ytaxi_data.join(weather, [weather.end_day>temp_ytaxi_data.tpep_pickup_datetime, \n",
    "                           weather.Date<=temp_ytaxi_data.tpep_pickup_datetime], how='full')\n",
    "    \n",
    "    temp_ytaxi_data = temp_ytaxi_data.join(sport, [sport.end_time>temp_ytaxi_data.tpep_pickup_datetime, \n",
    "                           sport.start_time<=temp_ytaxi_data.tpep_pickup_datetime], how='full')\n",
    "    \n",
    "    temp_HVFH_data = temp_HVFH_data.join(weather, [weather.end_day>temp_HVFH_data.pickup_datetime, \n",
    "                           weather.Date<=temp_HVFH_data.pickup_datetime], how='full')\n",
    "\n",
    "    temp_HVFH_data = temp_HVFH_data.join(sport, [sport.end_time>temp_HVFH_data.pickup_datetime, \n",
    "                           sport.start_time<=temp_HVFH_data.pickup_datetime], how='full')\n",
    "    \n",
    "    # obtain only selected columns\n",
    "    temp_ytaxi_data = temp_ytaxi_data.select([*ytaxi_required_col,\"TMid\", \"Weather\", \"day\", \"event\"])\n",
    "    temp_HVFH_data = temp_HVFH_data.select([*HVFH_required_col,\"TMid\", \"Weather\", \"day\", \"event\"])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # fill in event colunm, replace null values with 0\n",
    "    temp_HVFH_data = temp_HVFH_data.fillna(int(2),\"event\")\n",
    "    temp_ytaxi_data = temp_ytaxi_data.fillna(int(2),\"event\")\n",
    "    \n",
    "    \n",
    "    # exact date of the trip record is not useful now, as events, weather, temperature and day type is \n",
    "    # need to have every column in numeric form, change pick_up time from dataframe to int\n",
    "    \n",
    "    temp_HVFH_data = temp_HVFH_data.withColumn('pickup_datetime', \n",
    "                                               date_format('pickup_datetime', 'HH'))\n",
    "        \n",
    "    temp_ytaxi_data = temp_ytaxi_data.withColumn('tpep_pickup_datetime', \n",
    "                                                 date_format('tpep_pickup_datetime', 'HH'))\n",
    "\n",
    "    \n",
    "    # convert hours to int for future models\n",
    "    temp_HVFH_data = temp_HVFH_data.withColumn(\"pickup_datetime\", \n",
    "                                               col(\"pickup_datetime\").cast(\"int\"))\n",
    "    temp_ytaxi_data = temp_ytaxi_data.withColumn(\"tpep_pickup_datetime\", \n",
    "                                                 col(\"tpep_pickup_datetime\").cast(\"int\"))\n",
    "    \n",
    "    \n",
    "    # remove any null value from the dataset, since dataset is very large, this should not influence\n",
    "    # the data distribution\n",
    "    temp_HVFH_data = temp_HVFH_data.dropna('any')\n",
    "    temp_ytaxi_data = temp_ytaxi_data.dropna('any')\n",
    "    \n",
    "    \n",
    "    # export filtered file\n",
    "    temp_ytaxi_data.write.mode('overwrite').parquet(ytaxi_save_dir+\"final_\"+ytaxi_name[num]) \n",
    "    temp_HVFH_data.write.mode('overwrite').parquet(HVFH_save_dir+\"final_\"+HVFH_name[num]) \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dependent-riverside",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# sort sports and weather filenames so that when joining dataframes, monthly trip data \n",
    "# matches monthly weather and sport event data. \n",
    "\n",
    "sport_filename = []\n",
    "sport_work_dir = directory+\"/data/raw/sport/parquet/\"\n",
    "for filename in os.listdir(sport_work_dir):\n",
    "    if filename[-7:] == \"parquet\":\n",
    "        sport_filename.append(filename)\n",
    "# sort sport filename so that file sequence matches trip data month sequence\n",
    "sport_filename = sorted(sport_filename)\n",
    "\n",
    "\n",
    "weather_filename = []\n",
    "weather_work_dir = directory+\"/data/raw/weather/parquet/\"\n",
    "for filename in os.listdir(weather_work_dir):\n",
    "    if filename[-7:] == \"parquet\":\n",
    "        weather_filename.append(filename)\n",
    "# sort sport filename so that file sequence matches trip data month sequence\n",
    "weather_filename = sorted(weather_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "metric-deficit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16247.472387075424\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "# check total time for filtering and merging dataframe\n",
    "start = time.time()\n",
    "# now merge all monthly data, trip date + sport data + weather data\n",
    "# range is len(xx) since \n",
    "for i in range(0, len(ytaxi_name)):\n",
    "    # read sport and weather monthly parquet file\n",
    "    sport = spark.read.parquet(sport_work_dir+sport_filename[i], header=True)\n",
    "    weather = spark.read.parquet(weather_work_dir+weather_filename[i], header=True)\n",
    "\n",
    "    # if i is not len of list, it's a training dataset, use training directory\n",
    "    # len(ytaxi_name)-1 to mark the last file in filenames\n",
    "    if i != len(ytaxi_name)-1:\n",
    "        filter_file(Ytaxi_raw_train, HVFH_raw_train, i, sport, weather, \n",
    "                    filter_ytaxi_train, filter_HVFH_train)\n",
    "    \n",
    "    else:\n",
    "        filter_file(Ytaxi_raw_test, HVFH_raw_test, i, sport, weather, \n",
    "                    filter_ytaxi_test, filter_HVFH_test)\n",
    "    \n",
    "print(time.time() - start)\n",
    "\n",
    "# takes more than 4 hours to join and write all 8 month's data, on 8GB 4-core i5 8th generation\n",
    "# Jupyter notebook might create error due to memory usage, if happens, run each month's\n",
    "# data from below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "greatest-university",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.114586114883423\n"
     ]
    }
   ],
   "source": [
    "# need to join all training data, as spark cannot read folder of written parquet files (as it's also folder)\n",
    "# form \n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "check = 0\n",
    "\n",
    "final_ytaxi_directory = directory+\"/data/curated/ytaxi/train\"\n",
    "for filename in os.listdir(final_ytaxi_directory):\n",
    "    # check if file is parquet, and is not the fully converted file (in case)\n",
    "    # this code is run more than once\n",
    "    if filename[-7:] == \"parquet\" and filename[:4] != \"full\":\n",
    "        file_directory = final_ytaxi_directory +\"/\"+filename\n",
    "        # read file in train set\n",
    "        file = spark.read.parquet(file_directory)\n",
    "        if check == 0:\n",
    "            \n",
    "            # create a new spark dataframe that contains all read files from training\n",
    "            # with check == 0 , it means that this is the first file read, therefore there are no\n",
    "            # files to union with, make the final full data = to the first file\n",
    "            \n",
    "            final_ytaxi_train = file\n",
    "            \n",
    "            # we are sure there are files to union with\n",
    "            check = check + 1\n",
    "            continue\n",
    "            \n",
    "        final_ytaxi_train = final_ytaxi_train.union(file)\n",
    "        \n",
    "# write the full training file as parquet, same direction as all other files\n",
    "\n",
    "final_ytaxi_train.write.mode('overwrite').parquet(final_ytaxi_directory+\"/full_ytaxi_train.parquet\") \n",
    "        \n",
    "print(time.time() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "south-round",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.549479961395264\n"
     ]
    }
   ],
   "source": [
    "# do the same thing with HVFH trip data\n",
    "\n",
    "start = time.time()\n",
    "check = 0\n",
    "\n",
    "final_HVFH_directory = directory+\"/data/curated/HVFH/train\"\n",
    "for filename in os.listdir(final_HVFH_directory):\n",
    "    # check if file is parquet, and is not the fully converted file (in case)\n",
    "    # this code is run more than once\n",
    "    if filename[-7:] == \"parquet\" and filename[:4] != \"full\":\n",
    "        file_directory = final_HVFH_directory +\"/\"+filename\n",
    "        # read file in train set\n",
    "        file = spark.read.parquet(file_directory)\n",
    "        if check == 0:\n",
    "            \n",
    "            # create a new spark dataframe that contains all read files from training\n",
    "            # with check == 0 , it means that this is the first file read, therefore there are no\n",
    "            # files to union with, make the final full data = to the first file\n",
    "    \n",
    "            final_HVFH_train = file\n",
    "            \n",
    "            # we are sure there are files to union with\n",
    "            check = check + 1\n",
    "            continue\n",
    "            \n",
    "        final_HVFH_train = final_HVFH_train.union(file)\n",
    "        \n",
    "# write the full training file as parquet, same direction as all other files\n",
    "\n",
    "final_HVFH_train.write.mode('overwrite').parquet(final_HVFH_directory+\"/full_HVFH_train.parquet\") \n",
    "        \n",
    "print(time.time() - start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "protecting-intersection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>tpep_pickup_datetime</th><th>passenger_count</th><th>trip_distance</th><th>RatecodeID</th><th>PULocationID</th><th>DOLocationID</th><th>total_amount</th><th>trip_Time</th><th>TMid</th><th>Weather</th><th>day</th><th>event</th></tr>\n",
       "<tr><td>10</td><td>1.0</td><td>1.5</td><td>1.0</td><td>145</td><td>145</td><td>4.3</td><td>93</td><td>53.0</td><td>3</td><td>0</td><td>2</td></tr>\n",
       "<tr><td>10</td><td>1.0</td><td>1.5</td><td>1.0</td><td>145</td><td>145</td><td>3.8</td><td>4</td><td>53.0</td><td>3</td><td>0</td><td>2</td></tr>\n",
       "<tr><td>10</td><td>1.0</td><td>0.7</td><td>1.0</td><td>161</td><td>161</td><td>8.8</td><td>314</td><td>53.0</td><td>3</td><td>0</td><td>2</td></tr>\n",
       "<tr><td>10</td><td>1.0</td><td>2.0</td><td>1.0</td><td>163</td><td>141</td><td>15.3</td><td>634</td><td>53.0</td><td>3</td><td>0</td><td>2</td></tr>\n",
       "<tr><td>10</td><td>1.0</td><td>2.5</td><td>1.0</td><td>260</td><td>56</td><td>11.3</td><td>577</td><td>53.0</td><td>3</td><td>0</td><td>2</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+---------------+-------------+----------+------------+------------+------------+---------+----+-------+---+-----+\n",
       "|tpep_pickup_datetime|passenger_count|trip_distance|RatecodeID|PULocationID|DOLocationID|total_amount|trip_Time|TMid|Weather|day|event|\n",
       "+--------------------+---------------+-------------+----------+------------+------------+------------+---------+----+-------+---+-----+\n",
       "|                  10|            1.0|          1.5|       1.0|         145|         145|         4.3|       93|53.0|      3|  0|    2|\n",
       "|                  10|            1.0|          1.5|       1.0|         145|         145|         3.8|        4|53.0|      3|  0|    2|\n",
       "|                  10|            1.0|          0.7|       1.0|         161|         161|         8.8|      314|53.0|      3|  0|    2|\n",
       "|                  10|            1.0|          2.0|       1.0|         163|         141|        15.3|      634|53.0|      3|  0|    2|\n",
       "|                  10|            1.0|          2.5|       1.0|         260|          56|        11.3|      577|53.0|      3|  0|    2|\n",
       "+--------------------+---------------+-------------+----------+------------+------------+------------+---------+----+-------+---+-----+"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if file succesfully written\n",
    "test_ytaxi = spark.read.parquet(directory+\"/data/curated/ytaxi/train/full_ytaxi_train.parquet\")\n",
    "test_ytaxi.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "instant-census",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99757143"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if file succesfully written\n",
    "test_HVFH = spark.read.parquet(directory+\"/data/curated/HVFH/train/full_HVFH_train.parquet\")\n",
    "test_HVFH.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
