{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2022 Semester 1\n",
    "\n",
    "## Assignment 1: Naive Bayes Leaner for Adult Database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student Name(s):** Aobo Li\n",
    "<br>\n",
    "**Student ID(s):** 1172339"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Marking will be applied on the four functions that are defined in this notebook, and to your responses to the questions at the end of this notebook.\n",
    "\n",
    "## General info\n",
    "\n",
    "<b>Due date</b>: Friday, 8 April 2022 5pm\n",
    "\n",
    "<b>Submission method</b>: Canvas submission\n",
    "\n",
    "<b>Submission materials</b>: This iPython notebook is a template which you will use for your Assignment 1 submission. You need to only submitted the completed copy of this iPython notebook.\n",
    "\n",
    "<b>Late submissions</b>: -10% per day up to 5 days (both weekdays and weekends count). Submissions more than 5 days late will not be accepted (resul in a mark of 0).\n",
    "<ul>\n",
    "    <li>one day late, -1.0;</li>\n",
    "    <li>two days late, -2.0;</li>\n",
    "    <li>three days late, -3.0;</li>\n",
    "    <li>four days late, -4.0;</li>\n",
    "    <li>five days late, -5.0;</li>\n",
    "</ul>\n",
    "\n",
    "<b>Extensions</b>: Students who are demonstrably unable to submit a full solution in time due to medical reasons or other trauma, may apply for an extension.  In these cases, you should email <a href=\"mailto:ni.ding@unimelb.edu.au\">Ni Ding</a> as soon as possible after those circumstances arise. If you attend a GP or other health care service as a result of illness, be sure to provide a Health Professional Report (HPR) form (get it from the Special Consideration section of the Student Portal), you will need this form to be filled out if your illness develops into something that later requires a Special Consideration application to be lodged. You should scan the HPR form and send it with the extension requests.\n",
    "\n",
    "<b>Marks</b>: This assignment will be marked out of 20, and make up 20% of your overall mark for this subject.\n",
    "\n",
    "<b>Materials</b>: Use Jupyter Notebook and Python page on Canvas for information on the basic setup required for this class, including an iPython notebook viewer and the python packages NLTK, Numpy, Scipy, Matplotlib, Scikit-Learn. You can use any Python built-in packages, but do not use any other 3rd party packages; if your iPython notebook doesn't run on the marker's machine, you will lose marks. <b> You should use Python 3</b>.  \n",
    "\n",
    "\n",
    "<b>Evaluation</b>: Your iPython notebook should run end-to-end without any errors in a reasonable amount of time, and you must follow all instructions provided below, including specific implementation requirements and instructions for what needs to be printed (please avoid printing output we don't ask for). You should edit the sections below where requested, but leave the rest of the code as is. You should leave the output from running your code in the iPython notebook you submit, to assist with marking. The amount each section is worth is given in parenthesis after the instructions. \n",
    "\n",
    "You will be marked not only on the correctness of your methods, but also the quality and efficency of your code: in particular, you should be careful to use Python built-in functions and operators when appropriate and pick descriptive variable names that adhere to <a href=\"https://www.python.org/dev/peps/pep-0008/\">Python style requirements</a>. If you think it might be unclear what you are doing, you should comment your code to help the marker make sense of it. We reserve the right to deduct up to 2 marks for unreadable or exessively inefficient code.\n",
    "\n",
    "8 of the marks available for this Project will be assigned to whether the five specified Python functions work in a manner consistent with the materials from COMP90049. Any other implementation will not be directly assessed (except insofar as it is required to make these five functions work correctly).\n",
    "\n",
    "12 of the marks will be assigned to your responses to the questions, in terms of both accuracy and insightfulness. We will be looking for evidence that you have an implementation that allows you to explore the problem, but also that you have thought deeply about the data and the behaviour of the Naive Bayes classifier.\n",
    "\n",
    "<b>Updates</b>: Any major changes to the assignment will be announced via Canvas. Minor changes and clarifications will be announced on the discussion board (ED -> Assignments -> A1); we recommend you check it regularly.\n",
    "\n",
    "<b>Academic misconduct</b>: For most people, collaboration will form a natural part of the undertaking of this homework, and we encourge you to discuss it in general terms with other students. However, this ultimately is still an individual task, and so reuse of code or other instances of clear influence will be considered cheating. Please check the <a href=\"https://canvas.lms.unimelb.edu.au/courses/124196/modules#module_662096\">CIS Academic Honesty training</a> for more information. We will be checking submissions for originality and will invoke the University’s <a href=\"http://academichonesty.unimelb.edu.au/policy.html\">Academic Misconduct policy</a> where inappropriate levels of collusion or plagiarism are deemed to have taken place.\n",
    "\n",
    "**IMPORTANT**\n",
    "\n",
    "Please carefully read and fill out the <b>Authorship Declaration</b> form at the bottom of the page. Failure to fill out this form results in the following deductions: \n",
    "<UL TYPE=”square”>\n",
    "<LI>missing Authorship Declaration at the bottom of the page, -5.0\n",
    "<LI>incomplete or unsigned Authorship Declaration at the bottom of the page, -3.0\n",
    "</UL>\n",
    "**NOTE: COMPLETE AND SUBMIT THIS FILE. YOU SHOULD IMPLEMENT FOUR FUNCTIONS AND INCLUDE YOUR ANSWERS TO THE QUESTIONS IN THIS FILE ONLY. NO OTHER SUBMISSION IS REQUIRED.**\n",
    "\n",
    "**Keep your code clean. Adding proper comments to your code is MANDATORY.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Base code [8 marks]\n",
    "\n",
    "Instructions\n",
    "1. Do **not** shuffle the data set\n",
    "2. Treat the attributes as they are(e.g., do **not** convert numeric attributes to categorical or categorical to numeric). Implement a Naive Bayes classifier with appropriate likelihood function for each attribute.\n",
    "3. You should implement the Naive Bayes classifier from scratch. Do **not** use existing implementations/learning algorithms.\n",
    "4. You CONNOT have more than one train or predict function. Both continuous numeric attributes and categorical ones should be trained in one `train()` function, similarly for the `predict()`.  \n",
    "5. Apart from the instructions in point 3, you may use libraries to help you with data reading, representation, maths or evaluation\n",
    "6. Ensure that all and only required information is printed, as indicated in the final three code cells. Failure to adhere to print the required information will result in **[-1 mark]** per case. *(We don't mind details like you print a list or several numbers -- just make sure the information is displayed so that it's easily accessible)\n",
    "7. You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find. \n",
    "8. You should add adecuate comments to make your code easily comprehendible.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should prepare the data by reading it from a file and converting it into a useful format for training and testing\n",
    "# and implement 90-10 splitting as specified in the project description.\n",
    "\n",
    "# import tain test split form sklearn to split attributes and label into training and testing group\n",
    "def preprocess(filename, train_size):\n",
    "    # create empty list to store x - instances, and y - labels\n",
    "    x = []\n",
    "    y = []\n",
    "    # read the file in the same folder as this file, \n",
    "    with open(filename, mode='r') as f:\n",
    "        for line in f:\n",
    "            # split each line of data by , for better format\n",
    "            atts = line.strip().split(\",\")\n",
    "            x.append(atts[0:-1]) # all atts, excluding the label\n",
    "            y.append(atts[-1])   # contains only labels\n",
    "    # get all heading of the csv file\n",
    "    all_class = x[0]\n",
    "    # exclude heading from attributes and labels \n",
    "    x_value = x[1:]\n",
    "    y_value = y[1:]\n",
    "    num_of_train_data = int(len(x)*train_size)\n",
    "\n",
    "    \n",
    "    # split attributes and labels into 90:10 training and testing group, name them x_train.....\n",
    "    # x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.1,train_size=0.9, shuffle=False)\n",
    "    x_train = x_value[:num_of_train_data]\n",
    "    y_train = y_value[:num_of_train_data]\n",
    "    x_test = x_value[num_of_train_data:]\n",
    "    y_test = y_value[num_of_train_data:]\n",
    "\n",
    "    # return variables\n",
    "    return x_train, x_test, y_train, y_test, all_class, x_value, y_value\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This function should calculat prior probabilities and likelihoods (conditional probabilities) from the training data and using\n",
    "# to build a naive Bayes model\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "# define function that retrives nominal data\n",
    "def obtain_nominal_variable(numeric,attribute_list, all_class):\n",
    "    temp_list = []\n",
    "     # column 1 4 and 10 are numeric values, do not contain in nominal variables\n",
    "    for i in range(len(all_class)):\n",
    "        if i in numeric:\n",
    "            # column is numeric, focus only on nominal data\n",
    "            continue\n",
    "        else:\n",
    "            # create list of variable in that column\n",
    "            a = []\n",
    "            # obtain all variable in that attribute column\n",
    "            for j in range(len(attribute_list)):\n",
    "                a.append(attribute_list[j][i])\n",
    "            # set the list to get one instance of each, then convert to list to append to variable\n",
    "            temp_list.append(list(set(a)))\n",
    "    return temp_list\n",
    "\n",
    "\n",
    "# define function that retrives numeric data\n",
    "def obtain_numeric_variable(numeric,concept_list):\n",
    "    temp_list = []\n",
    "    # column 1 4 and 10 are numeric values, do not contain in nominal variables\n",
    "    for i in range(0,11):\n",
    "        if i in numeric:\n",
    "            b = []\n",
    "            for j in range(len(concept_list)):\n",
    "                # convert all value into int rather than string for future processes\n",
    "                b.append(int(concept_list[j][i]))\n",
    "            temp_list.append(b)\n",
    "    return temp_list\n",
    "\n",
    "\n",
    "\n",
    "# define function that counts occurence of variable in an attribute \n",
    "def count_variable(nominal,variable_list,count_list):\n",
    "    all_dict={}\n",
    "    # go through all class(heading), each heading is a dictionary containing counts of variables\n",
    "    for column_num in range(len(nominal)):\n",
    "        # create temperary dictionary for each heading\n",
    "        temp_dict = {}\n",
    "        \n",
    "        # go through set variable of x_train, count occurrance in less_50k dataframe, nominal column, \n",
    "        # dict keys are all_variable[i][j], len(all_variable[i]) is the number of variable in that heading\n",
    "        for variable_num in range(len(variable_list[column_num])):\n",
    "            # count the first occurence\n",
    "            count = count_list[nominal[column_num]].value_counts()\n",
    "            num = count[variable_list[column_num][variable_num]]\n",
    "            temp_dict[variable_list[column_num][variable_num]] = int(num)\n",
    "        # append this variable count for one heading to less or equal than 50k salary dictionary\n",
    "        all_dict[all_class[nominal[column_num]]] = temp_dict\n",
    "    \n",
    "    return all_dict\n",
    "\n",
    "\n",
    "# define function that adds all variable in attributes even with 0 count\n",
    "def add_all_var_count(all_variable,dictionary,all_class, nominal):\n",
    "    # go through 2d list of variable, attribute[variable]\n",
    "    for i in range(len(all_variable)):\n",
    "        # go through variable in that specific attribute\n",
    "        for j in range(len(all_variable[i])):\n",
    "            # if key is not fount in dictionary, then add this key in with count of 0\n",
    "            if all_variable[i][j] not in dictionary[all_class[nominal[i]]]:\n",
    "                dictionary[all_class[nominal[i]]][all_variable[i][j]] = 0\n",
    "    # return new dictionary with 0 count of variables\n",
    "    return dictionary\n",
    "\n",
    "    \n",
    "# calculate numeric estimators, mean, standard deviations for future gaussian distribution calculation\n",
    "def numeric_estimator(numeric_data, all_class, numeric):\n",
    "    #create a dictionary to store mean and standard deviation for all numeric attribute\n",
    "    dictionary = {}\n",
    "    # go through all three numeric attribute\n",
    "    for i in range(len(numeric_data)):\n",
    "        # create a temperary dictionary to store mean and sd for each attribute\n",
    "        temp = {}\n",
    "        # get sum of one attribute and instance to find mean\n",
    "        a = 0\n",
    "        # b is number of instance\n",
    "        n = 0\n",
    "        # get the sum of all numeric data, run a if line to check if there are missing values\n",
    "        for j in range(len(numeric_data[i])):\n",
    "            if type(numeric_data[i][j])==int:\n",
    "                a+=numeric_data[i][j]\n",
    "                n+=1\n",
    "        if n == 0:\n",
    "            break\n",
    "        mean = a/n\n",
    "        # s is sum of square of  (xi - mean)\n",
    "        s = 0\n",
    "        temp['mean'] = mean\n",
    "        for j in range(n):\n",
    "            s+=(numeric_data[i][j]-mean)**2\n",
    "        # unbiased estimator of variance requires n-1 \n",
    "        sd = (s/(n-1))**(1/2)\n",
    "        temp['sd'] = sd\n",
    "        # allocate attribute name with it's mean and variance\n",
    "        dictionary[all_class[numeric[i]]] = temp\n",
    "    return dictionary\n",
    "    \n",
    "    \n",
    "def laplace_smooth(data_type, all_class, all_variable, dictionary):\n",
    "    # apply laplace smoothing, run through all variable count in an attribute, if one variable in an attribute\n",
    "    # has a count of 0, all variables in that attribute will add 1, if not, continue\n",
    "    for i in range(len(data_type)):\n",
    "        cls = all_class[data_type[i]]\n",
    "        # check represents if variable equals to 0, 1 is none, 0 is at least 1\n",
    "        for j in range(len(dictionary[cls])):\n",
    "            variable = all_variable[i][j]\n",
    "            dictionary[cls][variable]+=1\n",
    "    return dictionary\n",
    "\n",
    " \n",
    "# define function that calculates probability of variable in an attribute after smoothing\n",
    "def calc_prob(nominal,all_class,all_variable,x_train,count_of_calculate_variable,\n",
    "              count_of_assist_variable):\n",
    "    # create a new dictionary to hold probabilities\n",
    "    dictionary_of_prob = {}\n",
    "    # go through all nominal count\n",
    "    for i in range(len(nominal)):\n",
    "        # create temperary dict to store all variable probability for one attribute\n",
    "        temp_dict = {}\n",
    "        # let a be attribute that is going to go through\n",
    "        a = all_class[nominal[i]]\n",
    "        # let b be variable of this attribute\n",
    "        b = all_variable[i]\n",
    "        # go through all values in attribute heading\n",
    "        num_of_instance = 0\n",
    "        for j in range(len(count_of_calculate_variable[a])):\n",
    "            num_of_instance += count_of_calculate_variable[a][b[j]]\n",
    "        for j in range(len(count_of_calculate_variable[a])):\n",
    "            \n",
    "            # let c be num of instance of variable\n",
    "            c = count_of_calculate_variable[a][b[j]]\n",
    "            \n",
    "            # d be num of instance of other count of variable\n",
    "            d = count_of_assist_variable[a][b[j]]\n",
    "            \n",
    "            # f be probability of attribute\n",
    "            f = (c+d)/(num_of_instance)\n",
    "            \n",
    "            # since all variable need to devide by prior, it doesn't matter if\n",
    "            # prior is devided in conditional probability\n",
    "            prob = (c/(c+d))*f\n",
    "            \n",
    "            temp_dict[b[j]]=prob\n",
    "        dictionary_of_prob[a]=temp_dict\n",
    "    return dictionary_of_prob\n",
    "\n",
    "\n",
    "def train(x_train, x_test, y_train, y_test, all_class):\n",
    "    # get the overall probability of <=50k and >50k\n",
    "    concept = list(set(y_train))\n",
    "    if '<' in concept[0]:\n",
    "        less = str(concept[0])\n",
    "        more = str(concept[1])\n",
    "    else:\n",
    "        less = str(concept[1])\n",
    "        more = str(concept[0])\n",
    "\n",
    "    priorless = y_train.count(less)/len(y_train)\n",
    "                                  \n",
    "    priormore = 1 - priorless\n",
    "\n",
    "    # now attribute values are obtained, create two dictionary of <=50 and >50 values, along with count of each variable\n",
    "    less_than_50k=[]\n",
    "    more_than_50k=[]\n",
    "    # go through all training data, seperate into two groups\n",
    "    for i in range(len(x_train)):\n",
    "        if y_train[i] == less:\n",
    "            less_than_50k.append(x_train[i])\n",
    "        else:\n",
    "            more_than_50k.append(x_train[i])\n",
    "            \n",
    "    #convert into data frame so column count is easier\n",
    "    less_50k = pd.DataFrame.from_records(less_than_50k)\n",
    "    more_50k = pd.DataFrame.from_records(more_than_50k)\n",
    "    \n",
    "    # create list that indicates numeric column number \n",
    "    numeric = []\n",
    "    nominal = []\n",
    "    example_instance = []\n",
    "    for i in range(1,len(x_train)):\n",
    "        if x_train[i].count(\"?\") == 0:\n",
    "            example_instance.append(x_train[i])\n",
    "            break\n",
    "\n",
    "    for i in range(len(example_instance[0])):\n",
    "        if example_instance[0][i].isdigit():\n",
    "            numeric.append(i)\n",
    "        else:\n",
    "            nominal.append(i)\n",
    "\n",
    "    # get all attribute variable from training data for two dataframe and the whole training set\n",
    "    all_variable =obtain_nominal_variable(numeric,x_train, all_class)\n",
    "    variable_less_50k= obtain_nominal_variable(numeric,less_than_50k, all_class)\n",
    "    variable_more_50k= obtain_nominal_variable(numeric,more_than_50k, all_class)\n",
    "    \n",
    "    # now convert dataframe into dictionary with counts of each column's variable and it's count\n",
    "    dict_less_50k = count_variable(nominal,variable_less_50k,less_50k)\n",
    "    dict_more_50k = count_variable(nominal,variable_more_50k,more_50k)\n",
    "    \n",
    "    \n",
    "    # for variables that is not in keys of dict, add those variable into dictionary with count 0\n",
    "    count_less_50k = add_all_var_count(all_variable, dict_less_50k, all_class, nominal)\n",
    "    count_more_50k = add_all_var_count(all_variable, dict_more_50k, all_class, nominal)\n",
    "\n",
    "    \n",
    "    # it might be possible that <=50K or >50K do not have attribute variables that exist in the other,\n",
    "    # run through all variable in training set, then smoothing the count of data by laplace so probability is\n",
    "    # easier to obtain later\n",
    "    count_less_50k = laplace_smooth(nominal, all_class, all_variable, count_less_50k)\n",
    "    count_more_50k = laplace_smooth(nominal, all_class, all_variable, count_more_50k)\n",
    "\n",
    "\n",
    "    # obtain numeric data and its mean and variance\n",
    "    numeric_less_50k = obtain_numeric_variable(numeric,less_than_50k)\n",
    "    numeric_more_50k = obtain_numeric_variable(numeric,more_than_50k)\n",
    "    # obtain mean and variance of numeric data for gaussian pdf\n",
    "    numeric_more_50k_estimator = numeric_estimator(numeric_more_50k, all_class, numeric)   \n",
    "    numeric_less_50k_estimator = numeric_estimator(numeric_less_50k, all_class, numeric)\n",
    "    \n",
    "    # compute conditional probability, get number of instance in each concept, <=50k and >50k\n",
    "    \n",
    "    # p(c|x) = p(x|c)p(c) / p(x) ||| p(attribute|>50k) = p(>50k|attribute)*p(attribute)/p(>50k)\n",
    "    # p attribute = count attribute / total number of instance\n",
    "    \n",
    "    prob_less_50k = calc_prob(nominal,all_class,all_variable,x_train,count_less_50k,\n",
    "              count_more_50k)\n",
    "    prob_more_50k = calc_prob(nominal,all_class,all_variable,x_train,count_more_50k,\n",
    "              count_less_50k)\n",
    "    \n",
    "\n",
    "    # return the needed information for prediction\n",
    "    return prob_less_50k, prob_more_50k, numeric_more_50k_estimator, numeric_less_50k_estimator, priorless, numeric_less_50k, numeric_more_50k, less, more\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should predict classes for new items in the testing data\n",
    "import math\n",
    "import numpy as np\n",
    "def predict(prob_less_50k, prob_more_50k, numeric_more_50k_estimator, numeric_less_50k_estimator,\n",
    "           x_test, y_test, all_class, prior,numeric_less_50k, numeric_more_50k, numeric_calculation_type,\n",
    "           less, more, sd):\n",
    "    # create list that indicates numeric and nominal column number \n",
    "    numeric = []\n",
    "    nominal = []\n",
    "    example_instance = []\n",
    "    # obtain an full instance without missing value\n",
    "    for i in range(len(x_train)):\n",
    "        if x_train[i].count(\"?\") == 0:\n",
    "            example_instance.append(x_train[i])\n",
    "            break\n",
    "    # check numeric and nominal column\n",
    "    for i in range(len(example_instance[0])):\n",
    "        if example_instance[0][i].isdigit():\n",
    "            numeric.append(i)\n",
    "        else:\n",
    "            nominal.append(i)\n",
    "    \n",
    "    \n",
    "    prediction = []\n",
    "    probability = []\n",
    "    # run through the test values\n",
    "    for i in range (len(x_test)):\n",
    "        #set probability as 0, since the c hat equals to log of prior + sum of log(conditional probability)\n",
    "        prob_less = 0\n",
    "        prob_more = 0\n",
    "        prob_less += math.log(prior)\n",
    "        prob_more += math.log((1-prior))\n",
    "        \n",
    "        instance = x_test[i]\n",
    "        # run through test data\n",
    "        # x_test[i] is one instance of test data\n",
    "\n",
    "        for j in range(len(instance)):\n",
    "            # go through all attribute variable of an instance\n",
    "            # if instance[j] where j is in numeric column number, apply gaussian/KDE pdf probability\n",
    "            if j in numeric:\n",
    "                # let attribute name be cls\n",
    "                cls = all_class[j]\n",
    "                x = float(instance[j])\n",
    "                # check which type of numeric calculation is needed\n",
    "                if numeric_calculation_type == \"Gaussian\" and '?' not in instance[j]:\n",
    "                    \n",
    "                    # go through more than 50k numeric probability (Gaussian)\n",
    "                    mean_more = numeric_more_50k_estimator[cls]['mean']\n",
    "                    sd_more = numeric_more_50k_estimator[cls]['sd']\n",
    "                    p_more = 1/(sd_more*math.sqrt(2*np.pi))*np.exp(-0.5*((x-mean_more)/sd_more)**2)\n",
    "                    prob_more += math.log(p_more)\n",
    "                    \n",
    "                    # go through less than 50k numeric probability (Gaussian)\n",
    "                    mean_less = numeric_less_50k_estimator[cls]['mean']\n",
    "                    sd_less = numeric_less_50k_estimator[cls]['sd']\n",
    "                    p_less = 1/(sd_less*math.sqrt(2*np.pi))*np.exp(-0.5*((x-mean_less)/sd_less)**2)\n",
    "                    prob_less += math.log(p_less)\n",
    "                    \n",
    "                \n",
    "                elif numeric_calculation_type == \"KDE\" and '?' not in instance[j]:\n",
    "                    # set mean and sd as 0 and 3\n",
    "                    mean = 0\n",
    "                    # kde_less and more is the sum of gaussian with x-xi\n",
    "                    kde_less = 0\n",
    "                    kde_more = 0\n",
    "                    # KDE = prior * 1/N * sum of gaussian * (x - xi), obtain \n",
    "                    # get list of that attribute in training\n",
    "                    train_less = numeric_less_50k[numeric.index(j)]\n",
    "                    train_more = numeric_more_50k[numeric.index(j)]\n",
    "                    # go through KDE sum process\n",
    "                    for k in range(len(train_less)):\n",
    "                        kde_less += 1/(sd*math.sqrt(2*np.pi))*np.exp(-0.5*((x-train_less[k])/sd)**2)\n",
    "                    for k in range(len(train_more)):\n",
    "                        kde_more += 1/(sd*math.sqrt(2*np.pi))*np.exp(-0.5*((x-train_more[k])/sd)**2)\n",
    "                    \n",
    "                    # finalise KDE probability calculation\n",
    "                    kde_less_prob = math.log(kde_less/len(train_less))\n",
    "                    kde_more_prob = math.log(kde_more/len(train_more))\n",
    "                    # probability will need to be in log form, as the product of all the probabilityies might \n",
    "                    # be too small for the computer to obtain\n",
    "                    prob_less += kde_less_prob\n",
    "                    prob_more += kde_more_prob\n",
    "                \n",
    "                else:\n",
    "                    # in case calculation type is wrong, terminate the program, provide avaliable calculation type\n",
    "                    print(\"Wrong Calculation type, only accept Gaussian/KDE\")\n",
    "                    exit()\n",
    "                \n",
    "            # nominal data\n",
    "            else:\n",
    "                attribute = all_class[j]\n",
    "                variable = x_test[i][j]\n",
    "                # check if variable in test data is a key in probability dictionary, if it does not exist in\n",
    "                # the dictionary, ignore this attribute instance\n",
    "                if prob_more_50k[attribute].get(variable) and prob_less_50k[attribute].get(variable)is not None:\n",
    "                    # probability will need to be in log form, as the product of all the probabilityies might \n",
    "                    # be too small for the computer to obtain\n",
    "                    prob_more += math.log(prob_more_50k[attribute].get(variable))\n",
    "                    prob_less += math.log(prob_less_50k[attribute].get(variable))\n",
    "        \n",
    "        # predict concept, the larger one is more possible\n",
    "        if prob_less > prob_more:\n",
    "            prediction.append(less)\n",
    "            probability.append([prob_less, prob_more])\n",
    "        else:\n",
    "            prediction.append(more)\n",
    "            probability.append([prob_less, prob_more])\n",
    "    \n",
    "    # return the list of predicted concepts and it's probability\n",
    "    return prediction, probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should evaliate the prediction performance by comparing your model’s class outputs to ground\n",
    "# truth labels, return and output accuracy, confusion matrix and F1 score.\n",
    "\n",
    "def evaluate(prediction, y_test, less, more):\n",
    "    # <=50K is positive, >50K is negative\n",
    "    total = len(prediction)\n",
    "    # True positive, prediction <=50K, test<=50K\n",
    "    TP = 0\n",
    "    # true negative, prediction >50K, test >50k\n",
    "    TN = 0\n",
    "    # false positive, prediction >50k, test <=50k\n",
    "    FP =0\n",
    "    # false negative, prediction <=50k, test >50k\n",
    "    FN = 0\n",
    "    for i in range(total):\n",
    "        if prediction[i] == y_test[i] == less:\n",
    "            TP+=1\n",
    "        if prediction[i] == y_test[i] == more:\n",
    "            TN+=1\n",
    "        if prediction[i] == less and y_test[i] == more:\n",
    "            FP+=1\n",
    "        if prediction[i] == more and y_test[i] == less:\n",
    "            FN+=1\n",
    "    \n",
    "    \n",
    "    accuracy = (TP+TN)/total\n",
    "    precision = TP/(TP+FP)\n",
    "    recall = TP/(TP+FN)\n",
    "    f_score = (2*precision*recall)/(precision+recall)\n",
    "    print(\"The accuracy of the prediction is: \", accuracy*100, \"%\")\n",
    "    # Now print out the confusion matrix\n",
    "    print(\"confusion matrix\")\n",
    "    print(\"\\t\\tprediction\")\n",
    "    print(\"\\tpositive\\tnegative\\n\")\n",
    "    print(\"positive  \", TP, \"\\t\\t\", FN, \"\\n\")\n",
    "    print(\"negative  \", FP, \"\\t\\t\", TN, \"\\n\")\n",
    "    print(\"The F1 score of the prediction is: \", f_score, '\\n\\n\\n')\n",
    "    \n",
    "    return accuracy, precision, recall, f_score, TP, FN, FP, TN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the prediction is:  86.0 %\n",
      "confusion matrix\n",
      "\t\tprediction\n",
      "\tpositive\tnegative\n",
      "\n",
      "positive   69 \t\t 8 \n",
      "\n",
      "negative   6 \t\t 17 \n",
      "\n",
      "The F1 score of the prediction is:  0.9078947368421053 \n",
      "\n",
      "\n",
      "\n",
      "Feature vectors of instances [0, 1, 2]:  \n",
      " ['68', ' ?', ' 1st-4th', '2', ' Divorced', ' ?', ' Not-in-family', ' White', ' Female', '20', ' United-States'] \n",
      " ['39', ' State-gov', ' Bachelors', '13', ' Never-married', ' Adm-clerical', ' Not-in-family', ' White', ' Male', '40', ' United-States'] \n",
      " ['50', ' Self-emp-not-inc', ' Bachelors', '13', ' Married-civ-spouse', ' Exec-managerial', ' Husband', ' White', ' Male', '13', ' United-States']\n",
      "\n",
      "\n",
      "\n",
      "Number of instances (N):  1000\n",
      "Number of features (F):  11\n",
      "Number of labels (L):  2\n",
      "\n",
      "Predicted class probabilities for instance N-3:  { <=50k: -20.768165488458912,  >50k: -19.762177925498445 }\n",
      "Predicted class ID for instance N-3:   >50K\n",
      "\n",
      "Predicted class probabilities for instance N-2:  { <=50k: -25.310387321914636,  >50k: -22.803496285083625 }\n",
      "Predicted class ID for instance N-2:   >50K\n",
      "\n",
      "Predicted class probabilities for instance N-1:  { <=50k: -16.929982096849525,  >50k: -16.919159161000110 }\n",
      "Predicted class ID for instance N-1:   >50K\n"
     ]
    }
   ],
   "source": [
    "# This cell should act as your \"main\" function where you call the above functions \n",
    "# on the full ADULT data set, and print the evaluation score. [0.33 marks]\n",
    "\n",
    "\n",
    "# First, read in the data and apply your NB model to the OBJECTIVITY data\n",
    "# change the file name if needed, default as \"adult.csv\"\n",
    "x_train, x_test, y_train, y_test, all_class, x, y= preprocess(\"adult.csv\", 0.9)\n",
    "# get the classes of the concept\n",
    "concept = list(set(y_train))\n",
    "\n",
    "\n",
    "# now train the data using train function, which returns probability for less\n",
    "# and more than 40k data, numeric column list, its mean and sd and prior of the data\n",
    "prob_less_50k, prob_more_50k, numeric_more_50k_estimator, numeric_less_50k_estimator, priorless, numeric_less_50k, numeric_more_50k, less, more= train(\n",
    "    x_train, x_test, y_train, y_test, all_class)\n",
    "\n",
    "\n",
    "# get the prediction of test data via prediction function\n",
    "prediction, probability = predict(prob_less_50k, prob_more_50k, numeric_more_50k_estimator, numeric_less_50k_estimator,\n",
    "           x_test, y_test, all_class, priorless,numeric_less_50k, numeric_more_50k, \"Gaussian\", less, more, None)\n",
    "\n",
    "\n",
    "# Second, print the full evaluation results from the evaluate() function\n",
    "accuracy, precision, recall, f_score, TP, FN, FP, TN = evaluate(prediction, y_test, less, more)\n",
    "\n",
    "\n",
    "\n",
    "# Third, print data statistics and model predictions, as instructed below \n",
    "# N is the total number of instances, F the total number of features, L the total number of labels\n",
    "# The \"class probabilities\" may be unnormalized\n",
    "# The \"predicted class ID\" must be in range (0, L)\n",
    "\n",
    "print(\"Feature vectors of instances [0, 1, 2]: \", \"\\n\",x_train[0],\"\\n\",x_train[1],\"\\n\",x_train[2])\n",
    "\n",
    "print(\"\\n\\n\\nNumber of instances (N): \", len(x_train)+len(x_test))\n",
    "print(\"Number of features (F): \", len(all_class))\n",
    "print(\"Number of labels (L): \", len(concept))\n",
    "\n",
    "\n",
    "# probability is the sum of logs, as it may be too small to obtain by the coomputer, take 15 places after\n",
    "# decimal point\n",
    "\n",
    "print(\"\\nPredicted class probabilities for instance N-3: \",\"{ <=50k: %.15f, \" %probability[-3][0],\n",
    "     \">50k: %.15f }\" %probability[-3][1])\n",
    "print(\"Predicted class ID for instance N-3: \", prediction[-3])\n",
    "print(\"\\nPredicted class probabilities for instance N-2: \", \"{ <=50k: %.15f, \" %probability[-2][0],\n",
    "     \">50k: %.15f }\" %probability[-2][1])\n",
    "print(\"Predicted class ID for instance N-2: \", prediction[-2])\n",
    "print(\"\\nPredicted class probabilities for instance N-1: \", \"{ <=50k: %.15f, \" %probability[-1][0],\n",
    "     \">50k: %.15f }\" %probability[-1][1])\n",
    "print(\"Predicted class ID for instance N-1: \", prediction[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Conceptual questions [8 marks for groups of 1] / [16 marks for groups of 2]\n",
    "\n",
    "\n",
    "If you are in a group of 1, you should respond to Q1 and Q2.\n",
    "\n",
    "If you are in a group of 2, you should respond to Q1, Q2, Q3 and Q4.\n",
    "\n",
    "A response to a question should take about 100–250 words. You may need to develope codes or functions to help respond to the question here. \n",
    "\n",
    "#### NOTE: We strongly recommend <u>including figures or tables, etc.</u> to support your responses. The figures and tables inserted in Markdown cells must be reproducable by your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 [4 marks]\n",
    "<u>Sensitivity</u> and <u>specificity</u> are two model evaluation metrics.  A good model should have both sensitivity and specificity high. Use the $2 \\times 2$ confusion matrix returned by `evaluate()` to calculate the sensitivity and specificity. Do you see a difference between them? If so, what causes this difference? Provide suggestions to improve the model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity of the first test is:  0.8961038961038961\n",
      "Specificity of the first test is:  0.7391304347826086\n",
      "Number of <=50K data in training:  692\n",
      "Number of >50K data in training:   208\n"
     ]
    }
   ],
   "source": [
    "# Write additional code here, if necessary (you may insert additional code cells)\n",
    "# sensitivity = TP/(TP+FN)\n",
    "# specificity = TN/(TN+FP)\n",
    "\n",
    "print(\"Sensitivity of the first test is: \", 69/(69+8))\n",
    "print(\"Specificity of the first test is: \", 17/(17+6))\n",
    "print(\"Number of <=50K data in training: \", len(numeric_less_50k[0]))\n",
    "print(\"Number of >50K data in training:  \", len(numeric_more_50k[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensitivity of the test is much higher than specificity, which suggests that this model can obtain high proportion of True positives cases, but also obtain more False positive cases, fewer false >50k is detected when the actural concept is <=50k. The main reason for this difference could be because the model is obtained through an unbalanced dataset, that it dies not have enough information for >50K prediction, as it only obtains 208 instances in training, very little comparing to 692 instance of <=50K data in training. It may also be caused by the missing value in training data, where I have used missing data as a new category of attribute variable to calculate the probability, reduced the true probability of >50k data. \n",
    "\n",
    "To solve this problem, more >50K data is needed in training, however, considering that the total number of >50k data is significantly less than <=50k, this approach may not be effective. One possible way is to classify dataset into <= and >50k first, then take 95% of the two group for training, the remaining 5% as testing. This approach might slightly increase specificity.\n",
    "\n",
    "Another way to solve this problem is to create a sub-dataset with even instance of classes, so that prior does not have influence on attribute's prediction. But the problem for this approach is that, prior may reflect the population and not considering it might decrease accuracy of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 [4 marks]\n",
    "You can adopt different methods for training and/or testing, which will produce different results in model evaluation. \n",
    "\n",
    "(a) Instead of Gaussian, <u>implement KDE</u> for  $P(X_i|c_j)$ for numeric attributes $X_i$. Compare the evaluation results with Gaussian. Which one do you think is more suitable to model $P(X_i|c_j)$, Gaussian or KDE? Observe all numeric attributes and justify your answer.\n",
    "\n",
    "You can choose an arbitrary value for kernel bandwidth $\\sigma$ for KDE, but a value between 3 and 15 is recommended. You should write code to implement KDE, not call an existing function/method such as `KernelDensity` from `scikit-learn`.\n",
    "\n",
    "(b) Implement <u>10-fold and 2-fold cross-validations</u>.  \n",
    "\tObserve the evaluation results in each fold and the average accuracy, recall and specificity over all folds. \n",
    "\tComment on what is the effect by changing the values of $m$ in $m$-fold cross validation. (You can choose either Gaussian or KDE Naive Bayes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the prediction is:  85.0 %\n",
      "confusion matrix\n",
      "\t\tprediction\n",
      "\tpositive\tnegative\n",
      "\n",
      "positive   68 \t\t 9 \n",
      "\n",
      "negative   6 \t\t 17 \n",
      "\n",
      "The F1 score of the prediction is:  0.9006622516556292 \n",
      "\n",
      "\n",
      "\n",
      "The accuracy of the prediction is:  84.0 %\n",
      "confusion matrix\n",
      "\t\tprediction\n",
      "\tpositive\tnegative\n",
      "\n",
      "positive   67 \t\t 10 \n",
      "\n",
      "negative   6 \t\t 17 \n",
      "\n",
      "The F1 score of the prediction is:  0.8933333333333333 \n",
      "\n",
      "\n",
      "\n",
      "The accuracy of the prediction is:  82.0 %\n",
      "confusion matrix\n",
      "\t\tprediction\n",
      "\tpositive\tnegative\n",
      "\n",
      "positive   64 \t\t 13 \n",
      "\n",
      "negative   5 \t\t 18 \n",
      "\n",
      "The F1 score of the prediction is:  0.8767123287671235 \n",
      "\n",
      "\n",
      "\n",
      "The accuracy of the prediction is:  82.0 %\n",
      "confusion matrix\n",
      "\t\tprediction\n",
      "\tpositive\tnegative\n",
      "\n",
      "positive   64 \t\t 13 \n",
      "\n",
      "negative   5 \t\t 18 \n",
      "\n",
      "The F1 score of the prediction is:  0.8767123287671235 \n",
      "\n",
      "\n",
      "\n",
      "The accuracy of the prediction is:  82.0 %\n",
      "confusion matrix\n",
      "\t\tprediction\n",
      "\tpositive\tnegative\n",
      "\n",
      "positive   64 \t\t 13 \n",
      "\n",
      "negative   5 \t\t 18 \n",
      "\n",
      "The F1 score of the prediction is:  0.8767123287671235 \n",
      "\n",
      "\n",
      "\n",
      "The accuracy of the prediction is:  82.0 %\n",
      "confusion matrix\n",
      "\t\tprediction\n",
      "\tpositive\tnegative\n",
      "\n",
      "positive   64 \t\t 13 \n",
      "\n",
      "negative   5 \t\t 18 \n",
      "\n",
      "The F1 score of the prediction is:  0.8767123287671235 \n",
      "\n",
      "\n",
      "\n",
      "The accuracy of the prediction is:  82.0 %\n",
      "confusion matrix\n",
      "\t\tprediction\n",
      "\tpositive\tnegative\n",
      "\n",
      "positive   64 \t\t 13 \n",
      "\n",
      "negative   5 \t\t 18 \n",
      "\n",
      "The F1 score of the prediction is:  0.8767123287671235 \n",
      "\n",
      "\n",
      "\n",
      "The accuracy of the prediction is:  83.0 %\n",
      "confusion matrix\n",
      "\t\tprediction\n",
      "\tpositive\tnegative\n",
      "\n",
      "positive   64 \t\t 13 \n",
      "\n",
      "negative   4 \t\t 19 \n",
      "\n",
      "The F1 score of the prediction is:  0.8827586206896552 \n",
      "\n",
      "\n",
      "\n",
      "The accuracy of the prediction is:  82.0 %\n",
      "confusion matrix\n",
      "\t\tprediction\n",
      "\tpositive\tnegative\n",
      "\n",
      "positive   63 \t\t 14 \n",
      "\n",
      "negative   4 \t\t 19 \n",
      "\n",
      "The F1 score of the prediction is:  0.8750000000000001 \n",
      "\n",
      "\n",
      "\n",
      "The accuracy of the prediction is:  82.0 %\n",
      "confusion matrix\n",
      "\t\tprediction\n",
      "\tpositive\tnegative\n",
      "\n",
      "positive   63 \t\t 14 \n",
      "\n",
      "negative   4 \t\t 19 \n",
      "\n",
      "The F1 score of the prediction is:  0.8750000000000001 \n",
      "\n",
      "\n",
      "\n",
      "The accuracy of the prediction is:  82.0 %\n",
      "confusion matrix\n",
      "\t\tprediction\n",
      "\tpositive\tnegative\n",
      "\n",
      "positive   63 \t\t 14 \n",
      "\n",
      "negative   4 \t\t 19 \n",
      "\n",
      "The F1 score of the prediction is:  0.8750000000000001 \n",
      "\n",
      "\n",
      "\n",
      "The accuracy of the prediction is:  82.0 %\n",
      "confusion matrix\n",
      "\t\tprediction\n",
      "\tpositive\tnegative\n",
      "\n",
      "positive   63 \t\t 14 \n",
      "\n",
      "negative   4 \t\t 19 \n",
      "\n",
      "The F1 score of the prediction is:  0.8750000000000001 \n",
      "\n",
      "\n",
      "\n",
      "The accuracy of the prediction is:  82.0 %\n",
      "confusion matrix\n",
      "\t\tprediction\n",
      "\tpositive\tnegative\n",
      "\n",
      "positive   63 \t\t 14 \n",
      "\n",
      "negative   4 \t\t 19 \n",
      "\n",
      "The F1 score of the prediction is:  0.8750000000000001 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "KDE estimation with sd=3 is:\n",
      "The accuracy of the prediction is:  85.0 %\n",
      "confusion matrix\n",
      "\t\tprediction\n",
      "\tpositive\tnegative\n",
      "\n",
      "positive   68 \t\t 9 \n",
      "\n",
      "negative   6 \t\t 17 \n",
      "\n",
      "The F1 score of the prediction is:  0.9006622516556292 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write additional code here, if necessary (you may insert additional code cells)\n",
    "# 2a\n",
    "# create a list to obtain all kde accuracy with different standard deviation\n",
    "accuracy_kde= []\n",
    "\n",
    "sd = 3\n",
    "# get all accuracy for kde estimation with different standard deviation\n",
    "for i in range(3,16):\n",
    "    # KDE is written in predict function, change calculation type into \"KDE\"\n",
    "    kde_prediction, kde_probability = predict(prob_less_50k, prob_more_50k, numeric_more_50k_estimator, numeric_less_50k_estimator,\n",
    "               x_test, y_test, all_class, priorless,numeric_less_50k, numeric_more_50k, \"KDE\", less, more, sd)\n",
    "    accuracy_k, precision_k, recall_k, f_score_k, TP_k, FN_k, FP_k, TN_k = evaluate(kde_prediction, y_test, less, more)\n",
    "    accuracy_kde.append(accuracy_k)\n",
    "    sd+=1\n",
    "\n",
    "# obtain only sd 3's kde data\n",
    "\n",
    "print(\"\\n\\nKDE estimation with sd=3 is:\")\n",
    "kde_prediction, kde_probability = predict(prob_less_50k, prob_more_50k, numeric_more_50k_estimator, numeric_less_50k_estimator,\n",
    "               x_test, y_test, all_class, priorless,numeric_less_50k, numeric_more_50k, \"KDE\", less, more, 3)\n",
    "accuracy_k, precision_k, recall_k, f_score_k, TP_k, FN_k, FP_k, TN_k = evaluate(kde_prediction, y_test, less, more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the prediction is:  79.0 %\n",
      "confusion matrix\n",
      "\t\tprediction\n",
      "\tpositive\tnegative\n",
      "\n",
      "positive   57 \t\t 18 \n",
      "\n",
      "negative   3 \t\t 22 \n",
      "\n",
      "The F1 score of the prediction is:  0.8444444444444444 \n",
      "\n",
      "\n",
      "\n",
      "The accuracy of the prediction is:  81.0 %\n",
      "confusion matrix\n",
      "\t\tprediction\n",
      "\tpositive\tnegative\n",
      "\n",
      "positive   62 \t\t 16 \n",
      "\n",
      "negative   3 \t\t 19 \n",
      "\n",
      "The F1 score of the prediction is:  0.8671328671328672 \n",
      "\n",
      "\n",
      "\n",
      "The accuracy of the prediction is:  79.0 %\n",
      "confusion matrix\n",
      "\t\tprediction\n",
      "\tpositive\tnegative\n",
      "\n",
      "positive   64 \t\t 13 \n",
      "\n",
      "negative   8 \t\t 15 \n",
      "\n",
      "The F1 score of the prediction is:  0.8590604026845637 \n",
      "\n",
      "\n",
      "\n",
      "The accuracy of the prediction is:  87.0 %\n",
      "confusion matrix\n",
      "\t\tprediction\n",
      "\tpositive\tnegative\n",
      "\n",
      "positive   69 \t\t 8 \n",
      "\n",
      "negative   5 \t\t 18 \n",
      "\n",
      "The F1 score of the prediction is:  0.9139072847682119 \n",
      "\n",
      "\n",
      "\n",
      "The accuracy of the prediction is:  86.0 %\n",
      "confusion matrix\n",
      "\t\tprediction\n",
      "\tpositive\tnegative\n",
      "\n",
      "positive   73 \t\t 7 \n",
      "\n",
      "negative   7 \t\t 13 \n",
      "\n",
      "The F1 score of the prediction is:  0.9125 \n",
      "\n",
      "\n",
      "\n",
      "The accuracy of the prediction is:  82.0 %\n",
      "confusion matrix\n",
      "\t\tprediction\n",
      "\tpositive\tnegative\n",
      "\n",
      "positive   66 \t\t 13 \n",
      "\n",
      "negative   5 \t\t 16 \n",
      "\n",
      "The F1 score of the prediction is:  0.88 \n",
      "\n",
      "\n",
      "\n",
      "The accuracy of the prediction is:  82.0 %\n",
      "confusion matrix\n",
      "\t\tprediction\n",
      "\tpositive\tnegative\n",
      "\n",
      "positive   67 \t\t 10 \n",
      "\n",
      "negative   8 \t\t 15 \n",
      "\n",
      "The F1 score of the prediction is:  0.881578947368421 \n",
      "\n",
      "\n",
      "\n",
      "The accuracy of the prediction is:  75.0 %\n",
      "confusion matrix\n",
      "\t\tprediction\n",
      "\tpositive\tnegative\n",
      "\n",
      "positive   62 \t\t 14 \n",
      "\n",
      "negative   11 \t\t 13 \n",
      "\n",
      "The F1 score of the prediction is:  0.832214765100671 \n",
      "\n",
      "\n",
      "\n",
      "The accuracy of the prediction is:  78.0 %\n",
      "confusion matrix\n",
      "\t\tprediction\n",
      "\tpositive\tnegative\n",
      "\n",
      "positive   63 \t\t 10 \n",
      "\n",
      "negative   12 \t\t 15 \n",
      "\n",
      "The F1 score of the prediction is:  0.8513513513513513 \n",
      "\n",
      "\n",
      "\n",
      "The accuracy of the prediction is:  86.0 %\n",
      "confusion matrix\n",
      "\t\tprediction\n",
      "\tpositive\tnegative\n",
      "\n",
      "positive   69 \t\t 8 \n",
      "\n",
      "negative   6 \t\t 17 \n",
      "\n",
      "The F1 score of the prediction is:  0.9078947368421053 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2b 10-fold cross validation\n",
    "# j indicates the starting point\n",
    "j = 0\n",
    "\n",
    "# create variable that contains sum of estimators, macro-averaging is applied as tests are large enough\n",
    "total_accuracy_10 = 0\n",
    "total_specificity_10 = 0\n",
    "total_fscore_10 = 0\n",
    "total_sensitivity_10 = 0\n",
    "total_precision_10 = 0\n",
    "\n",
    "# create variable that holds the size of fold\n",
    "fold_size10 = int(len(x)/10)\n",
    "\n",
    "# split into 10 list for x and y, take first 100, then second 100... as tests, other are training data\n",
    "# model goes for 10 times, same process as previous training\n",
    "for i in range(0,10):\n",
    "\n",
    "    cv_xtest = x[j:j+fold_size10]\n",
    "    cv_ytest = y[j:j+fold_size10]\n",
    "    cv_xtrain = x[0:j]+x[j+fold_size10:]\n",
    "    cv_ytrain = y[0:j]+y[j+fold_size10:]\n",
    "    \n",
    "    \n",
    "    # run through the same training procedures as before, replace train/test with new variable\n",
    "    prob_less_50k, prob_more_50k, numeric_more_50k_estimator, numeric_less_50k_estimator, priorless, numeric_less_50k, numeric_more_50k, less, more= train(cv_xtrain, \n",
    "                                                           cv_xtest, cv_ytrain, cv_ytest, all_class)\n",
    "    prediction, probability = predict(prob_less_50k, prob_more_50k, numeric_more_50k_estimator, numeric_less_50k_estimator,\n",
    "               cv_xtest, cv_ytest, all_class, priorless,numeric_less_50k, numeric_more_50k, \"Gaussian\", less, more, None)\n",
    "\n",
    "    # Print the full evaluation results from the evaluate() function\n",
    "    accuracy10, precision10, recall10, f_score10, TP10, FN10, FP10, TN10 = evaluate(prediction, cv_ytest, less, more)\n",
    "    \n",
    "    # calculate specificity as required\n",
    "    specificity10 = TN10/(TN10+FP10)\n",
    "    \n",
    "    # obtain all values estimators for average of these\n",
    "    total_accuracy_10 += accuracy10\n",
    "    total_specificity_10 +=specificity10\n",
    "    total_fscore_10 += f_score10\n",
    "    sensitivity10 = TP10/(TP10+FN10)\n",
    "    total_sensitivity_10 += sensitivity10\n",
    "    total_precision_10 += precision10\n",
    "    \n",
    "    # j adds fold size to move test data to next fold in the original list\n",
    "    j+=fold_size10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the prediction is:  82.19999999999999 %\n",
      "confusion matrix\n",
      "\t\tprediction\n",
      "\tpositive\tnegative\n",
      "\n",
      "positive   326 \t\t 61 \n",
      "\n",
      "negative   28 \t\t 85 \n",
      "\n",
      "The F1 score of the prediction is:  0.8798920377867745 \n",
      "\n",
      "\n",
      "\n",
      "The accuracy of the prediction is:  82.0 %\n",
      "confusion matrix\n",
      "\t\tprediction\n",
      "\tpositive\tnegative\n",
      "\n",
      "positive   335 \t\t 47 \n",
      "\n",
      "negative   43 \t\t 75 \n",
      "\n",
      "The F1 score of the prediction is:  0.8815789473684211 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The average data for 2 fold cross validation is\n",
      "Average accuracy:  0.821\n",
      "Average specificity:  0.693902804859757\n",
      "Average sensitivity:  0.8596703058836261\n",
      "Average precision:  0.903573670522823\n",
      "Average f score:  0.8807354925775979\n",
      "\n",
      "\n",
      "The average data for 10 fold cross validation is\n",
      "Average accuracy:  0.8150000000000001\n",
      "Average specificity:  0.7078850304285087\n",
      "Average sensitivity:  0.847512449866732\n",
      "Average precision:  0.9069893341782691\n",
      "Average f score:  0.8750084799692637\n"
     ]
    }
   ],
   "source": [
    "# 2b 2-fold cross validation\n",
    "# j indicates the starting point\n",
    "j = 0\n",
    "# create variable that contains sum of estimators, macro-averaging is applied as tests are large enough\n",
    "total_accuracy_2 = 0\n",
    "total_specificity_2 = 0\n",
    "total_fscore_2 = 0\n",
    "total_sensitivity_2 = 0\n",
    "total_precision_2 = 0\n",
    "\n",
    "# create variable that holds the size of fold\n",
    "fold_size2 = int(len(x)/2)\n",
    "\n",
    "# split into 2 list for x and y, take first 500, then second 500 as tests, other are training data\n",
    "# model runs twice, same process as previous training\n",
    "for i in range(0,2):\n",
    "    cv_xtest = x[j:j+fold_size2]\n",
    "    cv_ytest = y[j:j+fold_size2]\n",
    "    cv_xtrain = x[0:j]+x[j+fold_size2:]\n",
    "    cv_ytrain = y[0:j]+y[j+fold_size2:]\n",
    "    prob_less_50k, prob_more_50k, numeric_more_50k_estimator, numeric_less_50k_estimator, priorless, numeric_less_50k, numeric_more_50k, less, more= train(cv_xtrain, \n",
    "                                                           cv_xtest, cv_ytrain, cv_ytest, all_class)\n",
    "\n",
    "    prediction, probability = predict(prob_less_50k, prob_more_50k, numeric_more_50k_estimator, numeric_less_50k_estimator,\n",
    "                cv_xtest, cv_ytest, all_class, priorless,numeric_less_50k, numeric_more_50k, \"Gaussian\", less, more, None)\n",
    "    # Second, print the full evaluation results from the evaluate() function\n",
    "    accuracy2, precision2, recall2, f_score2, TP2, FN2, FP2, TN2 = evaluate(prediction, cv_ytest, less, more)\n",
    "    specificity2 = TN2/(TN2+FP2)\n",
    "\n",
    "    # add up all of the estimators of cross validation for average calculation\n",
    "    total_accuracy_2 += accuracy2 \n",
    "    total_specificity_2 += specificity2\n",
    "    total_fscore_2 += f_score2\n",
    "    sensitivity2 = TP2/(TP2+FN2)\n",
    "    \n",
    "    total_sensitivity_2 += sensitivity2\n",
    "    total_precision_2 += precision2\n",
    "    \n",
    "    # j adds fold size to move test data to next fold in the original list\n",
    "    j+=fold_size2\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "# print out the final average of the estimators for 2-fold and 10-fold cross validation\n",
    "# 2 fold data\n",
    "print(\"\\n\\nThe average data for 2 fold cross validation is\")\n",
    "print(\"Average accuracy: \", total_accuracy_2/2)\n",
    "print(\"Average specificity: \", total_specificity_2/2)\n",
    "print(\"Average sensitivity: \", total_sensitivity_2/2)\n",
    "print(\"Average precision: \", total_precision_2/2)\n",
    "print(\"Average f score: \", total_fscore_2/2)\n",
    "\n",
    "# 10-fold data\n",
    "print(\"\\n\\nThe average data for 10 fold cross validation is\")\n",
    "print(\"Average accuracy: \", total_accuracy_10/10)\n",
    "print(\"Average specificity: \", total_specificity_10/10)\n",
    "print(\"Average sensitivity: \", total_sensitivity_10/10)\n",
    "print(\"Average precision: \", total_precision_10/10)\n",
    "print(\"Average f score: \", total_fscore_10/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8, 0.95)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# now print graph comparing the KDE and Naive Bayes, increase bar chart size to include all bar plot\n",
    "# and their name\n",
    "plt.figure(figsize=(18, 5)) \n",
    "plt.bar(['kde accuracy', 'gaussian accuracy'],[accuracy_k,accuracy], width=0.5)\n",
    "plt.bar(['kde precision', 'gaussian precision'],[precision_k,precision], width=0.5)\n",
    "plt.bar(['kde recall', 'gaussian recall'],[recall_k,recall], width=0.5)\n",
    "plt.bar(['kde f1 score', 'gaussian f1 score'],[f_score_k,f_score], width=0.5)\n",
    "# limit the y axis from 0.8 to 1 to better show the data\n",
    "plt.title('KDE vs. Naive Bayes (2a)')\n",
    "plt.ylim([0.8, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print graph of KDE vs Gaussian distribution, with all standard deviation of KDE from 3 to 1\n",
    "gau_accuracy = [accuracy]*len(accuracy_kde)\n",
    "\n",
    "# create sd list from 3 to 15\n",
    "lst = list(range(3,16))\n",
    "# adjust size of graph for better view\n",
    "plt.figure(figsize=(18, 10)) \n",
    "plt.plot(lst,accuracy_kde, label = \"KDE accuracy\")\n",
    "plt.plot(lst,gau_accuracy, label = \"Gaussian accuracy\")\n",
    "plt.title('KDE vs. Gaussian accuracy (2a)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Standard deviation')\n",
    "plt.ylim([0.8, 0.88])\n",
    "plt.xticks(lst)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.65, 0.95)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJUAAAJOCAYAAAAOHTYIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA79UlEQVR4nO3deZx+dVk//tcluOWKSpbiWihSlhbaYiblRplZbuFSapb5/aWlaaVlRqh9aTPTLDMjXFJcU1SKSEVNTQFlERRF0gQ1cU2/5QJevz/OGbgZZuYzb/zcn5mB5/PxmMfc99nu6z7v+5z7zGve55zq7gAAAADAiCttdQEAAAAA7DxCJQAAAACGCZUAAAAAGCZUAgAAAGCYUAkAAACAYUIlAAAAAIYJlQCAJElVPb2qPlNVn9rEtB+tqrutM+7gqjp391d42VXV1avq9VX1xap65S6mvXlVdVXtvc74w6rqJcupdHtYfI9VddOq+nJV7bWraS/ja51RVQdf1vl3sexfqapnbXLaDd9HVb2nqr5rtxUHAJcDQiUA2KGq6jFVdVJVfbWqjlpj/F2r6oNV9T9V9ZaqutkGy7ppkickObC7v22JZe8WVfWbVfX+qvpSVf1HVf3mLma5f5IbJrl+dz9gD5R4udHd/9nd1+zuC7/ZZVXVUVX19FXL/67uPuGbXfYar3WVJE9J8ifz81tV1euq6vyq+lxVHVdVtx5Y5J8mOXx31wkAO5lQCQB2rk8keXqSI1ePqKobJHlNkt9Lcr0kJyV5+QbLummSz3b3p5dQ5zJUkl9Isk+SQ5I8pqoO3WD6myX5UHdfsCeK25WaOA5brvsk+WB3nzc/v26SY5LcOlPA+J4krxtY3jFJfqyqtn3oCgB7ioMZANihuvs13f3aJJ9dY/R9k5zR3a/s7q8kOSzJ91bVAasnnE9jOz7JjebTnI6ah//0fGrSF6rqhKq6zVp1zKeWHVVVn6+qM5PcYb2aq+qvq+pPVw17XVX9xvz4t6vqvLkH0llVddd13vsfd/d7u/uC7j4rUzhwp3Ve8w+SPDXJz83v75FVdaWqekpVfayqPl1VL6qq66wz/y2q6q1zTccnucF672+e/j5VdUpV/XdVfaSqDpmHn1BVz6iqdyT5nyS3rKofrqoT59PyTqyqH15YzsOr6pyF3lgPmYd/51zPF2s6XXHNsLCq/qmqHrNq2KlVdd/58V9U1cfnOk+uqjuvs5xLnA64q/VRVa+sqk/N9b1t5ZSxqnpUkock+a25HV4/D7/oVMqqumpVPauqPjH/PKuqrjqPO7iqzq2qJ8xt9smqesQGTfETSd668qS739Pdf9fdn+vuryf58yS3rqrrL8xztap6+fze3ltV37sw/1eSnJzknhu8JgBcoQiVAODy6buSnLrypLv/X5KPzMMvobv/NdMf4J+YT3N6eFXdKsnLkjwuyb5Jjk3y+ppOKVrt95N8x/xzzyQP26Cul2UKdypJqmqfJPdIcnRNpyI9Jskduvta87I+uqs3Oi/rzknOWGt8d/9+kj9M8vL5/f1dkofPPz+W5JZJrpnkL9d5iZdmChNukORpG72/qrpjkhcl+c1MPWN+dNV7+Pkkj0pyrSRfSvLGJM9Ocv0kz0zyxqq6flVdYx7+E/O6+OEkp8zLeFqSf8nUS2u/JM9Zp5yXJXnQQm0HZuqx9cZ50IlJbpepJ9tLk7yyqq623ntbsKv18U9J9k/yrUnem+QfkqS7nz8//uO5He69xrJ/N8kPznV9b5I7ZjqFbcW3JblOkhsneWSS586fobXcNslZG7yPH03yqe5eDGXvk+SVuXidvLaqrrww/gNzXQBAhEoAcHl1zSRfXDXsi5nCjM34uSRv7O7j514df5rk6pnCjdUemOQZcw+Qj2cKQ9bz9iSdKQRKpmsdvau7P5HkwiRXTXJgVV25uz/a3R/ZRK2HZTqm+ftNTLviIUme2d3ndPeXkzw5yaG16uLcNV1r6g5Jfq+7v9rdb0vy+g2W+8gkR87r7RvdfV53f3Bh/FHdfcZ8Gt49kny4u18897h6WZIPJlkJW76R5Lur6urd/cnuXgnNvp4pHLpRd3+lu/9tnVr+Mcnt6uJraT0kyWu6+6tJ0t0v6e7Pzq/9Z5nW/YbXGNrM+ujuI7v7S/PrHJaph9yavcDW8JAkh3f3p7v7/CR/kCmIW/H1efzXu/vYJF/eoObrZgru1nof+yV5bpLfWDXq5O5+1fyZf2aSq2UKuVZ8aV4uABChEgBcXn05ybVXDbt2ki9V1Z3n04++XFVr9u5JcqMkH1t50t3fSPLxTD1E1pr24wvPP7bGNCvL6SRH5+IeNA/OxT1Zzs7UM+qwJJ+uqqOr6kbrLSuZLlae6dpK91oJS6rqdxbe3/M28/7mx3tnutbO6uk+P/f02uX7S3KTTD3C1rO4nlbXsLLsG8+v93NJHp3kk1X1xrr41MXfynRNqffUdHriL671Qt290hNq5VpTD8q8rpOkqp5YVR+YT1P7QqYeQBue2pddrI+q2quqjphP+/vvXNxLa1fLXVz+6nZZ/Ax8dtV1sf4nU4C6ls9njRC1qvbN1NPrr+Ygb9FF7TN/5s9d9frXSvKFjd8CAFxxCJUA4PLpjCycpjOfTvUdma6z9Pb59KNrdvd6t0j/RKbeMCvzV6bA5Lw1pv3kPG7FTXdR28uS3H/uQfMDSV69MqK7X9rdPzK/dif5o/UWMocpT0py1+4+d2EZf7jw/h69mfc313xBkv9a473tM6+/zby/j2daz+vpDWpYWfZ5SdLdx3X33ZN8e6YeTH87D/9Ud/9yd98oya8k+auq+s51Xu9lSR5UVT+UqdfNW5Jkvn7Sb2XqZbZPd183U0+22qD2ZNfr48GZTiG7W6aQ6ubz8JXlLr7/tazVLp/YxTzrOS3JrRYHzKfK/UuSY7r7GWvMc5OFaa+U6fTCxde/TRZOKwWAKzqhEgDsUFW193wNnL2S7FVVV1s4fesfM506db95mqcmOW3VqVgbeUWSe1XVXedryjwhyVeTvHOdaZ9cVfvMpxU9dqMFd/f7knwmyQuSHNfdX5jfz62r6sfnCzN/Jcn/ZjoFbK33/pBM10m6e3efs8n3tOhlSR4/X3T6mrn4mkuXuDtcd38s053z/qCqrlJVP5KLT09by98lecS83q5UVTeuNS6OPjs2ya2q6sFzW/5ckgOTvKGqbljTBb+vkWm9fznzuqiqB8zrOZl643TWWU/za9wsyeHz+1uZ7lqZQrTzk+xdVU/NpXu2Xcom1se15no/m+RbMq3XRf+V6RpW63lZkqdU1b413cHwqUlesqu61nFskrusPKmqayc5Lsk7uvtJ68zz/VV133k7elym9/Lv8/xXS/L9mS5qDwBEqAQAO9lTMgUvT0ry0PnxU5Jkvh7N/ZI8I1Pw8AO5+DSoXZrvqPbQTBeB/kym4ODe3f21NSb/g0ynKf1Hpl4gL97ES7w0U2+Wly4Mu2qSI+bX+1SmCz0/eZ35n57p4tYnbuJUt7UcOdf5trnur2T9MOzBmdbf5zJdlPxF6y20u9+T5BGZ7iz2xUx3H1vdG2ll2s8m+alMgd1nM/Uc+qnu/kymY7TfyNRL5nOZwpH/M896hyTvrqovZ7rN/a+vF6zNpwS+Jpde18cl+eckH8rUdl/JJU/N28hG6+NF8/LOS3Jm5kBmwd9lumbWF6rqtWss++mZQqvTkpye6ULfT99kXau9PskBC6dQ/mymdfeIhc/Ml+frRK14XabTDj+f6VpO952vr5RM28AJ8/W/AIAkNV3aAAAALl+q6lFJDuzux+2GZb07ySO7+/3fdGEAcDkhVAIAAABg2FJPf6uqQ6rqrKo6u6oude56Vd2sqt5UVadV1QkL1wdIVV1YVafMP8css04AAAAAxiytp1JV7ZXpPP27Z7od64lJHtTdZy5M88okb+juF1bVjyd5RHf//Dzuy9293i1iAQAAANhCy+ypdMckZ3f3OfNFPY/OdIvZRQcmefP8+C1rjAcAAABgG9p715NcZjfOJe8icm6mO4UsOjXJfZP8RaY7clyrqq4/3w3lalV1Uqbb3R7R3a9d/QLzxRcflSTXuMY1vv+AA9a7Yy8AAAAAo04++eTPdPe+a41bZqi0GU9M8pdV9fBMt/Q9L8mF87ibdfd5VXXLJG+uqtO7+yOLM3f385M8P0kOOuigPumkk/Zc5QAAAACXc1X1sfXGLTNUOi/JTRae7zcPu0h3fyJTT6VU1TWT3K+7vzCPO2/+fU5VnZDk9kkuESoBAAAAsDWWeU2lE5PsX1W3qKqrJDk0ySXu4lZVN6iqlRqenOTIefg+VXXVlWmS3CnJmQEAAABgW1haqNTdFyR5TJLjknwgySu6+4yqOryqfnqe7OAkZ1XVh5LcMMkz5uG3SXJSVZ2a6QLeRyzeNQ4AAACArVXdvdU17BauqQQAAACwe1XVyd190Frjlnn6GwAAAACXU0IlAAAAAIYJlQAAAAAYJlQCAAAAYJhQCQAAAIBhQiUAAAAAhgmVAAAAABgmVAIAAABgmFAJAAAAgGFCJQAAAACGCZUAAAAAGCZUAgAAAGCYUAkAAACAYUIlAAAAAIYJlQAAAAAYJlQCAAAAYJhQCQAAAIBhQiUAAAAAhgmVAAAAABgmVAIAAABgmFAJAAAAgGFCJQAAAACGCZUAAAAAGCZUAgAAAGCYUAkAAACAYUIlAAAAAIYJlQAAAAAYJlQCAAAAYJhQCQAAAIBhQiUAAAAAhgmVAAAAABgmVAIAAABgmFAJAAAAgGFCJQAAAACGCZUAAAAAGCZUAgAAAGCYUAkAAACAYUIlAAAAAIYJlQAAAAAYJlQCAAAAYJhQCQAAAIBhQiUAAAAAhgmVAAAAABgmVAIAAABgmFAJAAAAgGFCJQAAAACGCZUAAAAAGCZUAgAAAGCYUAkAAACAYUIlAAAAAIYJlQAAAAAYJlQCAAAAYJhQCQAAAIBhQiUAAAAAhgmVAAAAABgmVAIAAABgmFAJAAAAgGFCJQAAAACGCZUAAAAAGCZUAgAAAGCYUAkAAACAYUIlAAAAAIYJlQAAAAAYJlQCAAAAYJhQCQAAAIBhQiUAAAAAhgmVAAAAABgmVAIAAABgmFAJAAAAgGFCJQAAAACGCZUAAAAAGCZUAgAAAGDY3ltdAMAecdh1trqC3e+wL251BQAAwBWYnkoAAAAADBMqAQAAADBMqAQAAADAMKESAAAAAMOESgAAAAAMc/c3ALYXd+oDAIAdQU8lAAAAAIYJlQAAAAAYJlQCAAAAYNhSQ6WqOqSqzqqqs6vqSWuMv1lVvamqTquqE6pqv4VxD6uqD88/D1tmnQAAAACMWVqoVFV7JXlukp9IcmCSB1XVgasm+9MkL+ru70lyeJL/O897vSS/n+QHktwxye9X1T7LqhUAAACAMcvsqXTHJGd39znd/bUkRye5z6ppDkzy5vnxWxbG3zPJ8d39ue7+fJLjkxyyxFoBAAAAGLDMUOnGST6+8PzcediiU5Pcd378s0muVVXX3+S8qapHVdVJVXXS+eefv9sKBwAAAGBjW32h7icmuUtVvS/JXZKcl+TCzc7c3c/v7oO6+6B99913WTUCAAAAsMreS1z2eUlusvB8v3nYRbr7E5l7KlXVNZPcr7u/UFXnJTl41bwnLLFWAAAAAAYss6fSiUn2r6pbVNVVkhya5JjFCarqBlW1UsOTkxw5Pz4uyT2qap/5At33mIcBAAAAsA0sLVTq7guSPCZTGPSBJK/o7jOq6vCq+ul5soOTnFVVH0pywyTPmOf9XJKnZQqmTkxy+DwMAAAAgG1gmae/pbuPTXLsqmFPXXj8qiSvWmfeI3NxzyUAAAAAtpGtvlA3AAAAADuQUAkAAACAYUIlAAAAAIYJlQAAAAAYJlQCAAAAYJhQCQAAAIBhQiUAAAAAhgmVAAAAABgmVAIAAABgmFAJAAAAgGFCJQAAAACGCZUAAAAAGCZUAgAAAGCYUAkAAACAYUIlAAAAAIYJlQAAAAAYJlQCAAAAYJhQCQAAAIBhQiUAAAAAhgmVAAAAABgmVAIAAABgmFAJAAAAgGFCJQAAAACGCZUAAAAAGCZUAgAAAGCYUAkAAACAYUIlAAAAAIYJlQAAAAAYJlQCAAAAYNjeW10AAAAAcMX13Ee/eatLWIpffd6Pb3UJS6enEgAAAADDhEoAAAAADBMqAQAAADBMqAQAAADAMKESAAAAAMOESgAAAAAMEyoBAAAAMEyoBAAAAMAwoRIAAAAAw/be6gIAAABgGZ776DdvdQm73a8+78e3ugS4iJ5KAAAAAAwTKgEAAAAwTKgEAAAAwDChEgAAAADDhEoAAAAADBMqAQAAADBMqAQAAADAMKESAAAAAMOESgAAAAAMEyoBAAAAMEyoBAAAAMAwoRIAAAAAw4RKAAAAAAwTKgEAAAAwTKgEAAAAwDChEgAAAADDhEoAAAAADBMqAQAAADBMqAQAAADAMKESAAAAAMOESgAAAAAMEyoBAAAAMEyoBAAAAMAwoRIAAAAAw4RKAAAAAAwTKgEAAAAwTKgEAAAAwLC9t7oAAACAneQDB9xmq0vY7W7zwQ9sdQnADqSnEgAAAADDhEoAAAAADBMqAQAAADBMqAQAAADAMKESAAAAAMOESgAAAAAMEyoBAAAAMEyoBAAAAMAwoRIAAAAAw4RKAAAAAAwTKgEAAAAwTKgEAAAAwDChEgAAAADDlhoqVdUhVXVWVZ1dVU9aY/xNq+otVfW+qjqtqn5yHn7zqvrfqjpl/nneMusEAAAAYMzey1pwVe2V5LlJ7p7k3CQnVtUx3X3mwmRPSfKK7v7rqjowybFJbj6P+0h3325Z9QEAAABw2S2zp9Idk5zd3ed099eSHJ3kPqum6STXnh9fJ8knllgPAAAAALvJMkOlGyf5+MLzc+dhiw5L8tCqOjdTL6XHLoy7xXxa3Fur6s5rvUBVPaqqTqqqk84///zdWDoAAAAAG9nqC3U/KMlR3b1fkp9M8uKqulKSTya5aXffPslvJHlpVV179czd/fzuPqi7D9p33333aOEAAAAAV2TLDJXOS3KThef7zcMWPTLJK5Kku9+V5GpJbtDdX+3uz87DT07ykSS3WmKtAAAAAAxYZqh0YpL9q+oWVXWVJIcmOWbVNP+Z5K5JUlW3yRQqnV9V+84X+k5V3TLJ/knOWWKtAAAAAAxY2t3fuvuCqnpMkuOS7JXkyO4+o6oOT3JSdx+T5AlJ/raqHp/pot0P7+6uqh9NcnhVfT3JN5I8urs/t6xaAQAAABiztFApSbr72EwX4F4c9tSFx2cmudMa8706yauXWRsAAGwnHzjgNltdwm53mw9+YKtLAGCJtvpC3QAAAADsQEIlAAAAAIYJlQAAAAAYJlQCAAAAYJhQCQAAAIBhQiUAAAAAhgmVAAAAABgmVAIAAABgmFAJAAAAgGFCJQAAAACGCZUAAAAAGCZUAgAAAGCYUAkAAACAYUIlAAAAAIYJlQAAAAAYJlQCAAAAYJhQCQAAAIBhQiUAAAAAhgmVAAAAABgmVAIAAABgmFAJAAAAgGFCJQAAAACGCZUAAAAAGCZUAgAAAGCYUAkAAACAYUIlAAAAAIYJlQAAAAAYJlQCAAAAYJhQCQAAAIBhQiUAAAAAhgmVAAAAABgmVAIAAABgmFAJAAAAgGFCJQAAAACGCZUAAAAAGCZUAgAAAGCYUAkAAACAYUIlAAAAAIYJlQAAAAAYJlQCAAAAYJhQCQAAAIBhQiUAAAAAhgmVAAAAABgmVAIAAABgmFAJAAAAgGFCJQAAAACGCZUAAAAAGLb3VhcAO9nNn/TGrS5ht/voEffa6hIAAADYAfRUAgAAAGCYUAkAAACAYUIlAAAAAIYJlQAAAAAYJlQCAAAAYJhQCQAAAIBhQiUAAAAAhgmVAAAAABgmVAIAAABgmFAJAAAAgGF7b3UBXNrNn/TGrS5hKT56xL22ugQAAABgN9FTCQAAAIBhQiUAAAAAhgmVAAAAABi2qVCpql5TVfeqKiEUAAAAAJvuqfRXSR6c5MNVdURV3XqJNQEAAACwzW0qVOruf+3uhyT5viQfTfKvVfXOqnpEVV15mQUCAAAAsP1s+nS2qrp+kocn+aUk70vyF5lCpuOXUhkAAAAA29bem5moqv4xya2TvDjJvbv7k/Ool1fVScsqDgAAAIDtaVOhUpK/7e5jFwdU1VW7+6vdfdAS6gIAAABgG9vs6W9PX2PYu3ZnIQAAAADsHBv2VKqqb0ty4yRXr6rbJ6l51LWTfMuSawMAAABgm9rV6W/3zHRx7v2SPHNh+JeS/M6SagIAAABgm9swVOruFyZ5YVXdr7tfvYdqAgAAAGCb29Xpbw/t7pckuXlV/cbq8d39zDVmAwAAAOByblenv11j/n3NZRcCAAAAwM6xq9Pf/mb+/Qd7phwAAAAAdoJdnf727I3Gd/ev7d5yAAAAANgJdnX628l7pAoAAAAAdpTN3P0NAAAAAC5hV6e/Pau7H1dVr0/Sq8d3908vrTIAAAAAtq1dnf724vn3ny67EAAAAAB2jl2d/nby/PutVXWVJAdk6rF0Vnd/bQ/UBwAAAMA2tKueSkmSqrpXkucl+UiSSnKLqvqV7v6nZRYHAAAAwPZ0pU1O92dJfqy7D+7uuyT5sSR/vquZquqQqjqrqs6uqietMf6mVfWWqnpfVZ1WVT+5MO7J83xnVdU9N/uGAAAAAFi+TfVUSvKl7j574fk5Sb600QxVtVeS5ya5e5Jzk5xYVcd095kLkz0lySu6+6+r6sAkxya5+fz40CTfleRGSf61qm7V3Rdusl4AAAAAlmhXd3+77/zwpKo6NskrMl1T6QFJTtzFsu+Y5OzuPmde1tFJ7pNkMVTqJNeeH18nySfmx/dJcnR3fzXJf1TV2fPy3rWZNwUAAADAcu2qp9K9Fx7/V5K7zI/PT3L1Xcx74yQfX3h+bpIfWDXNYUn+paoem+QaSe62MO+/r5r3xqtfoKoeleRRSXLTm950F+UAAAAAsLvs6u5vj1jy6z8oyVHd/WdV9UNJXlxV373Zmbv7+UmenyQHHXRQL6lGAAAAAFbZ7N3frpbkkZmucXS1leHd/YsbzHZekpssPN9vHrbokUkOmZf1rvl1brDJeQEAAADYIpu9+9uLk3xbknsmeWumkGfDC3VnuubS/lV1i6q6SqYLbx+zapr/THLXJKmq22QKrM6fpzu0qq5aVbdIsn+S92yyVgAAAACWbLN3f/vO7n5AVd2nu19YVS9N8vaNZujuC6rqMUmOS7JXkiO7+4yqOjzJSd19TJInJPnbqnp8pot2P7y7O8kZVfWKTBf1viDJr7rzGwAAAMD2sdlQ6evz7y/M1zz6VJJv3dVM3X1skmNXDXvqwuMzk9xpnXmfkeQZm6wPAAAAgD1os6HS86tqnyS/l+nUtGvOjwEAAAC4AtpUqNTdL5gfvjXJLZdXDgAAAAA7waYu1F1V16+q51TVe6vq5Kp6VlVdf9nFAQAAALA9bfbub0cn+XSS+yW5f5LPJHn5sooCAAAAYHvb7DWVvr27n7bw/OlV9XPLKAgAAACA7W+zPZX+paoOraorzT8PTHLcMgsDAAAAYPvasKdSVX0pSSepJI9L8pJ51JWSfDnJE5dZHAAAAADb04ahUndfa08VAgAAAMDOsdlrKqWqfjrJj85PT+juNyynJAAAAAC2u01dU6mqjkjy60nOnH9+var+7zILAwAAAGD72mxPpZ9Mcrvu/kaSVNULk7wvyZOXVRgAAAAA29dm7/6WJNddeHyd3VwHAAAAADvIZnsq/WGS91XVWzLdCe5HkzxpaVUBAAAAsK3tMlSqqisl+UaSH0xyh3nwb3f3p5ZZGAAAAADb1y5Dpe7+RlX9Vne/Iskxe6AmAAAAALa5zV5T6V+r6olVdZOqut7Kz1IrAwAAAGDb2uw1lX4uSSf5/1YNv+XuLQcAAACAnWCzodKBmQKlH8kULr09yfOWVRQAAAAA29tmQ6UXJvnvJM+enz94HvbAZRQFAAAAwPa22VDpu7v7wIXnb6mqM5dREAAAAADb32Yv1P3eqvrBlSdV9QNJTlpOSQAAAABsd5vtqfT9Sd5ZVf85P79pkrOq6vQk3d3fs5TqAAAAANiWNhsqHbLUKgAAAADYUTYVKnX3x5ZdCAAAAAA7x2avqQQAAAAAFxEqAQAAADBMqAQAAADAMKESAAAAAMOESgAAAAAMEyoBAAAAMEyoBAAAAMAwoRIAAAAAw4RKAAAAAAwTKgEAAAAwTKgEAAAAwDChEgAAAADDhEoAAAAADBMqAQAAADBMqAQAAADAMKESAAAAAMOESgAAAAAMEyoBAAAAMEyoBAAAAMAwoRIAAAAAw4RKAAAAAAwTKgEAAAAwTKgEAAAAwDChEgAAAADDhEoAAAAADBMqAQAAADBMqAQAAADAMKESAAAAAMOESgAAAAAMEyoBAAAAMEyoBAAAAMAwoRIAAAAAw4RKAAAAAAwTKgEAAAAwTKgEAAAAwDChEgAAAADDhEoAAAAADBMqAQAAADBMqAQAAADAMKESAAAAAMOESgAAAAAMEyoBAAAAMEyoBAAAAMAwoRIAAAAAw4RKAAAAAAwTKgEAAAAwTKgEAAAAwDChEgAAAADDhEoAAAAADBMqAQAAADBMqAQAAADAMKESAAAAAMOESgAAAAAMEyoBAAAAMEyoBAAAAMAwoRIAAAAAw4RKAAAAAAwTKgEAAAAwbKmhUlUdUlVnVdXZVfWkNcb/eVWdMv98qKq+sDDuwoVxxyyzTgAAAADG7L2sBVfVXkmem+TuSc5NcmJVHdPdZ65M092PX5j+sUluv7CI/+3u2y2rPgAAAAAuu2X2VLpjkrO7+5zu/lqSo5PcZ4PpH5TkZUusBwAAAIDdZJmh0o2TfHzh+bnzsEupqpsluUWSNy8MvlpVnVRV/15VP7POfI+apznp/PPP301lAwAAALAr2+VC3YcmeVV3X7gw7GbdfVCSByd5VlV9x+qZuvv53X1Qdx+077777qlaAQAAAK7wlhkqnZfkJgvP95uHreXQrDr1rbvPm3+fk+SEXPJ6SwAAAABsoWWGSicm2b+qblFVV8kUHF3qLm5VdUCSfZK8a2HYPlV11fnxDZLcKcmZq+cFAAAAYGss7e5v3X1BVT0myXFJ9kpyZHefUVWHJzmpu1cCpkOTHN3dvTD7bZL8TVV9I1PwdcTiXeMAAAAA2FpLC5WSpLuPTXLsqmFPXfX8sDXme2eS2y6zNgAAAAAuu+1yoW4AAAAAdhChEgAAAADDhEoAAAAADBMqAQAAADBMqAQAAADAMKESAAAAAMOESgAAAAAMEyoBAAAAMEyoBAAAAMAwoRIAAAAAw4RKAAAAAAwTKgEAAAAwTKgEAAAAwDChEgAAAADDhEoAAAAADBMqAQAAADBMqAQAAADAMKESAAAAAMOESgAAAAAMEyoBAAAAMEyoBAAAAMAwoRIAAAAAw4RKAAAAAAwTKgEAAAAwTKgEAAAAwDChEgAAAADDhEoAAAAADBMqAQAAADBMqAQAAADAMKESAAAAAMOESgAAAAAMEyoBAAAAMEyoBAAAAMAwoRIAAAAAw4RKAAAAAAwTKgEAAAAwTKgEAAAAwDChEgAAAADDhEoAAAAADBMqAQAAADBMqAQAAADAMKESAAAAAMOESgAAAAAMEyoBAAAAMEyoBAAAAMAwoRIAAAAAw4RKAAAAAAwTKgEAAAAwTKgEAAAAwDChEgAAAADDhEoAAAAADBMqAQAAADBMqAQAAADAMKESAAAAAMOESgAAAAAMEyoBAAAAMEyoBAAAAMAwoRIAAAAAw4RKAAAAAAwTKgEAAAAwTKgEAAAAwDChEgAAAADDhEoAAAAADBMqAQAAADBMqAQAAADAMKESAAAAAMOESgAAAAAMEyoBAAAAMEyoBAAAAMAwoRIAAAAAw/be6gIAgJ3nti+87VaXsBSnP+z0rS4BAGDH0FMJAAAAgGFCJQAAAACGCZUAAAAAGCZUAgAAAGCYUAkAAACAYUIlAAAAAIYJlQAAAAAYJlQCAAAAYJhQCQAAAIBhQiUAAAAAhi01VKqqQ6rqrKo6u6qetMb4P6+qU+afD1XVFxbGPayqPjz/PGyZdQIAAAAwZu9lLbiq9kry3CR3T3JukhOr6pjuPnNlmu5+/ML0j01y+/nx9ZL8fpKDknSSk+d5P7+segEAAADYvGX2VLpjkrO7+5zu/lqSo5PcZ4PpH5TkZfPjeyY5vrs/NwdJxyc5ZIm1AgAAADBgmaHSjZN8fOH5ufOwS6mqmyW5RZI3j8xbVY+qqpOq6qTzzz9/txQNAAAAwK5tlwt1H5rkVd194chM3f387j6ouw/ad999l1QaAAAAAKstM1Q6L8lNFp7vNw9by6G5+NS30XkBAAAA2MOWGSqdmGT/qrpFVV0lU3B0zOqJquqAJPskedfC4OOS3KOq9qmqfZLcYx4GAAAAwDawtLu/dfcFVfWYTGHQXkmO7O4zqurwJCd190rAdGiSo7u7F+b9XFU9LVMwlSSHd/fnllUrAAAAAGOWFiolSXcfm+TYVcOeuur5YevMe2SSI5dWHAAAAACX2Xa5UDcAAAAAO4hQCQAAAIBhQiUAAAAAhgmVAAAAABgmVAIAAABg2FLv/gYAwNa67Qtvu9UlLMXpDzt9q0sAgCs8PZUAAAAAGCZUAgAAAGCYUAkAAACAYUIlAAAAAIYJlQAAAAAYJlQCAAAAYJhQCQAAAIBhQiUAAAAAhgmVAAAAABgmVAIAAABgmFAJAAAAgGFCJQAAAACGCZUAAAAAGCZUAgAAAGCYUAkAAACAYUIlAAAAAIYJlQAAAAAYJlQCAAAAYJhQCQAAAIBhQiUAAAAAhgmVAAAAABgmVAIAAABgmFAJAAAAgGFCJQAAAACGCZUAAAAAGCZUAgAAAGCYUAkAAACAYUIlAAAAAIYJlQAAAAAYJlQCAAAAYJhQCQAAAIBhQiUAAAAAhgmVAAAAABgmVAIAAABgmFAJAAAAgGFCJQAAAACGCZUAAAAAGCZUAgAAAGCYUAkAAACAYUIlAAAAAIYJlQAAAAAYJlQCAAAAYJhQCQAAAIBhQiUAAAAAhgmVAAAAABgmVAIAAABgmFAJAAAAgGFCJQAAAACGCZUAAAAAGCZUAgAAAGCYUAkAAACAYUIlAAAAAIYJlQAAAAAYJlQCAAAAYJhQCQAAAIBhQiUAAAAAhgmVAAAAABgmVAIAAABgmFAJAAAAgGFCJQAAAACGCZUAAAAAGCZUAgAAAGCYUAkAAACAYUIlAAAAAIYJlQAAAAAYJlQCAAAAYJhQCQAAAIBhQiUAAAAAhgmVAAAAABgmVAIAAABgmFAJAAAAgGFCJQAAAACGCZUAAAAAGCZUAgAAAGCYUAkAAACAYUIlAAAAAIYtNVSqqkOq6qyqOruqnrTONA+sqjOr6oyqeunC8Aur6pT555hl1gkAAADAmL2XteCq2ivJc5PcPcm5SU6sqmO6+8yFafZP8uQkd+ruz1fVty4s4n+7+3bLqg8AAACAy26ZPZXumOTs7j6nu7+W5Ogk91k1zS8neW53fz5JuvvTS6wHAAAAgN2kuns5C666f5JDuvuX5uc/n+QHuvsxC9O8NsmHktwpyV5JDuvuf57HXZDklCQXJDmiu1+7xms8Ksmj5qe3TnLWUt7M5dsNknxmq4tgU7TVzqCddg5ttXNoq51DW+0c2mpn0E47h7baObTVuJt1975rjVja6W+btHeS/ZMcnGS/JG+rqtt29xcyFX1eVd0yyZur6vTu/sjizN39/CTP38M1X65U1UndfdBW18GuaaudQTvtHNpq59BWO4e22jm01c6gnXYObbVzaKvda5mnv52X5CYLz/ebhy06N8kx3f317v6PTL2W9k+S7j5v/n1OkhOS3H6JtQIAAAAwYJmh0olJ9q+qW1TVVZIcmmT1Xdxem6mXUqrqBkluleScqtqnqq66MPxOSc4MAAAAANvC0k5/6+4LquoxSY7LdL2kI7v7jKo6PMlJ3X3MPO4eVXVmkguT/GZ3f7aqfjjJ31TVNzIFX0cs3jWO3crpgzuHttoZtNPOoa12Dm21c2irnUNb7QzaaefQVjuHttqNlnahbgAAAAAuv5Z5+hsAAAAAl1NCJQAAAACGCZWWqKqOrKpPV9X7Vw2/XlUdX1Ufnn/vs878L6uq06rq8Ru8xlFVdf81hh9cVW/45t/FFUtV3aSq3lJVZ1bVGVX16+tMt29Vvbuq3ldVd95geR+dLza/evhhVfXE3Vn7FcUVabuqqkdX1S9sMP6nq+pJe6qey+ryul1V1Quq6sANxh9eVXfbU/WMuiJtS/Nr3qiqXrXw/KL6d9VWVXVQVT17fnzwfO3HPe7yui3Nr3nR/mx1/VV1bFVdd4N5L9pXVtXDq+pGe6js1XVc0bapdbeLXX1/zdNctA+tqt9ZbrXr1nB53qbW3S529f01T/PO+ffNq+rBy612zHbd1lZ9rzxg/kx9o6qu8Leu3+ptbY3vlWdU1cer6svf3DsjESot21FJDllj+JOSvKm790/ypvn5JVTVtyW5Q3d/T3f/+VKrXJKq2mura7gMLkjyhO4+MMkPJvnVdb5075rk9O6+fXe/fY9WuBvUZKdu/0dlB25Xl2Wdd/fzuvtFG4w/pruP+OarW7ptv11V1fCNK7r7lza6iUR3P7W7//Wbq2ypjsoO3JYuq+7+RHffP7l0/btqq+4+qbt/bX56cJItCZWyA7aly2rV/uwS9Xf3T3b3FzaYd3Ff+fAkWxIq5Yq3Ta27Xezq+2ueZnEfuiWhUi7f29S628Wuvr/maVba8+ZJtlWolG24ra2x3PcnuW+St+2u19hEDUu7CddusNXb2urlvj7JHXfj8ndpm7fPN6e7/SzxJ9OO+P2rhp2V5Nvnx9+e5Kw15jstyf8mOSXJnZPcLsm/z8P/Mck+83RHJbn//PiQJB9M8t4kz07yhnXqefs8zXuT/PDCuN9OcnqSUzPdcS9JvjPJv87D3pvkOzIdOLxhYb6/TPLw+fFHk/zRPO2hSX45yYnz/K9O8i3zdDec38ep888PJzk8yeMWlvuMJL++xe33uiR3XzXsdkn+M8n5c/tcPcmD5nX3/iR/tDDtR5PcYH78u0k+lOTfkrwsyRPXeL17J3l3kvfN6/2G8/BrJvn7+TVOS3K/hTZ/77wO3zQPO2xx2XNNN59/zkryoiRnJLlZkr9OctL8/A8W5rlDknfOy31Pkmtl+lK83cI0/5bke21XnUwHa69LckKSDyf5/YU6V6/z38y0TZy2ap3/wjzs1CQvXt2WSX4tyZnzNEcvvO5fLrzWm+fxb0py04X38uy5Pc9ZeV9XsO3qqCTPmz/rH0ryUwvr75h5vb01yTWSHJnpM/++JPeZp9sryZ/OdZyW5LHz8BOSHDSPP2oef3qSx6/xObrrvMzT59e46sJ7+YP583V6kgOu4NvSd83r/5R5WfvPNX4wyT8k+UCSV+Xi75Lvn9vu5Ex3lF2pe63vrove6xr1L9a51v7v4CRvmJfxqSTnLcz7H0muPM977cXnl8Nt6QHzMk5N8raF7ehS+7953EMX2vNvkuy18FlY/d318EzHE2vVv1jnuvvKJPdP8uVMn+FTktwryWsX6rl7kn+8om9Tu2ibL2c6/jp1fv2V45C12v7grL9drLTJAUnes2r9nD4/PiHTPvSITHeBPiXTdr5lx4PZHtvUXkn+JBcfK/zKwvo+IdM+cGWfuHLTpSNy8THCn+5iu7j6wrp/dJI/Wajn4bn4uOLL8+9/T/LFed7HZ5scD2b7bWuXWO7C8BOSHLTOe/j2eX2eMn8O7rzweqv3kddL8tr5df49yfcstPOLk7xj/pztm+lvrhPnnzvt6bbZbtvaWstdGPflDWq8yzz9KZmO4a41D1/rb+b1PkcnJHlWpmPQJ2Sd45ad/rPlBVzef7L2Du8LC49r8fl6880f0LvMjw9P8qz58VGZviyuluTjmQ7AK8krsvYO71uSXG1+vH+Sk+bHP5HpIHrlQP168+93J/nZ+fHV5vkPzsah0m8tjLv+wuOn5+I/xl6e+YAh05fndeb3/N552JWSfGRx/i1qu/9Mcu01xj08F3/p3miebt8ke2f6A/VnFtbHDeYdyOnz+rt2krOz9sHFPrn4AOGXkvzZ/PiPVtp8Ybp95za/xao2Oyzrh0rfSPKDC+NW5tkr007ve5JcJVP4cId53LXn9/Wwhc/drVY+O7ariz4Pn0xy/UxfgO/PdLB2iXWe5B6ZbmFa82f8DUl+NNNB/4dy8ZfjpdoyySdycRBx3TU+h69P8rD58S9m/kNqfi+vnF/vwCRnb1W7beF2dVSSf57Xwf5Jzp3b9uHz45X1/YdJHrqyjuc2uUaS/5PpIH7vVe1zwtzO35/k+IXXu+7C6y5+jm41D39RLt7/fTQX7xf/vyQvuIJvS89J8pD58VUybU83T9KZD4wzhXJPTHLlTN9b+87Dfy7JkfPjtb67Lqp5jfpX6lxv/3fwSr259D7273PxZ/NRmffbl9Nt6fQkN171OX941t7/3SbTfmklcPurTIHQet9dizVf9HhVnZvZV56Q+Q+4TJ+1Dy58Rl6a5N62qbXbZn7cK+soyR8necoGbX9w1t8uFtvklIX2/u2FZS621ZdXrYs9fjyY7bNNPWphHV010x+jt5jX9xeT7Devl3cl+ZFM295Zufj4cWU5i21w0bpefD6/h7MXhv9Tkh9ZbJNc+rh/WxwPZvtta5eqZ611v2rcE5L87vx4r0z/xFhvH/mcXPxPyx9PcspCO5+cOSjJtJ9bacObJvnAVrTPNtzWLlruquEbhUqvz8XHHteca1jvb+b1PkcnJPmr+fG6xy07/Wennv5yudHTJ6o3mqaqrpPpC+Kt86AXZvpDdNEBSf6juz88L/Ml6yzuykn+tqpOz/SH5kq3w7sl+fvu/p+5rs9V1bUyfdH94zzsKyvjd+HlC4+/u6rePr/eQzIdECbTzvCv5+Ve2N1f7O6PJvlsVd0+0x/f7+vuz27i9Xa7qrpmppT/cd3937uY/A5JTuju87v7gkz/OVrdPnfO9N/R/5mXd8w6y9ovyXHz+vrNXLy+7pbkuSsTdffnM3UdfVt3/8c87HObeGsf6+5/X3j+wKp6b6b0/bsyfR5uneST3X3ivNz/nt/XK5P8VFVdOVNocdQmXm9LbMF2lUyhwme7+3+TvCbTgV5yyXV+j/nnfZn+A3VApoOUH0/yyu7+zFz/Wm15WpJ/qKqHZupCvNoPZTqQSKb/WP3IwrjXdvc3eurqfsMN3sNSbeF2lSSvmNfBhzOFBgfMw49fWN/3SPKkqjol00HA1TIdkN0tyd/MdazVPuckuWVVPaeqDkmy+r3dOtPn6EPz89WftdfMv0/OdKC1bWzBtvSuJL9TVb+d5Gbz9pQkH+/ud8yPX5Lp833rJN+d5Pi5zZ6SZL9v4rsrWX//t5EXJHnE/PgRmUKmpdrCbekdSY6qql/O9AfQirX2f3fNdLB/4tw+d01yy1y2764Vm9lXXmT+rL04yUNruibTD2X6o3nLbJNtar22SZKvZfqHR3LJfdJ6bb8Zr8j0x1Pm3y/fYNpsxfHgNtum7pHkF+a2eXem0Gj/edx7uvvc7v5GprDu5pmCpq8k+buqum+Sze7v0t3nJzmnqn6wqq6f6XP1jl3MtiOOB7foWHDUiUkeUVWHJbltd38p6+8jfyTT/izd/eYk16+qa8/jjln4vrxbkr+cPz/HJLn2/PneFrb4WHDUO5I8s6p+LdPn5IKs/Tfzrj5HK/u8NY9bdmO9W+bye17f9vZfVfXt3f3Jqvr2JJ/eg6/9+CT/leR7M/2X4yuXYRkX5JLX47raqvH/b+HxUZkS5VOr6uGZ/tuxkRdkSpK/LdN/o/e4+Uvy1Un+obtfs6vpd7PnJHlmdx9TVQdn+u/DqI3a56K2qapbZPpv/x26+/NVdVQu3ZYX6e7/qarjk9wnyQMzHZBuJ1u5XSWXPnBZeb64PVSS/9vdf7M4YVU9dhPLv1emL6h7J/ndqrrtQG1fXVXDHrfF21Wy+fa5X3eftThh1carbN5+vjfJPTOdSvDATAfam7XSPhdme3wvb9m21N0vrap3Z/q8H1tVv5IptFur/SrJGd39Q4sj5lBpj+nud9R0IduDM51C9P5dzPJN2cptqbsfXVU/kKl9Tq6qle+B9drnhd395MURVXXv5Vd6CX+f6b/NX8kUSO0qJFyG7bZNrdk2s6/PfzgnC/ukDdp+M16e5JVV9ZppUf3hTcyzx44Ht+E2VZl6sB63qs6Dc8nv8wsz9aC9oKrumCkcvH+Sx2QKYDfr6EzfWx/M9Mf5hkHMNj8e3OpjwSHd/baq+tFM7X9UVT0zyecvw6IWj2WulKmH/GX5G2+ptsGx4JDuPqKq3pjkJ5O8o6rueRkXtdI+ax63XB7oqbQ1jsnUdTTz79dtNHF3fzHJ5+viK+D/fKZzMRd9MMnNq+o75ucPWmdx18n0X9hvzMtZ+a/I8ZmS8m9JprsnzGn5uVX1M/Owq87jP5bkwPn5dTN9ia3nWkk+Oe9EHrIw/E2ZTilJVe01J7zJdA7qIZmS6Ut8me4JNf31+HeZuoo+c5OzvSfJXarqBjVdnPxBuXT7vC3Jz1TV1ec/eNY7qL5OpmsSJBd/RpKpfX51oc59Mp23+6NzOJSqut48+qNJvm8e9n2Zukyv5dqZdnJfrKobZurOmczno1fVHeZlXGvhwnIvyHQu+Yk99ZbaTrZyu0qSu9d015GrJ/mZrP2fvuOS/OLKf4yq6sZV9a2ZuvM+YP4v4WJbZn5+pSQ36e63ZDp14DqZuuEuemem65gl07a2bS4kug22q2Rav1ea2/KWmT7nqx2X5LFzvZn/S55M29+vrGwHa7TPDZJcqbtfnem/Tt+3arlnZfocfef8fK3P2nayZdtSVd0yyTnd/ez5db9nHnXTqlo5CHtwpusmnJVk35XhVXXlqvquDb67NmOj/d+KL2X6blv0okw9BZfaS2mrt6Wq+o7ufnd3PzXTtSluMo9aa//3piT3n/dxK3dlulnW/+7ajA33lbNLtE93fyLT6cNPyR7oRbaO7bZNrdc269qg7VestV2svJ+PZApAfi/r91L6+nysuGKPHA9u023quCT/Z2V9VNWtquoaG7yHaya5Tncfm+mfx9+7xmTrtk+mdX2f+X0cvcl5t+vx4FYfCw6Zt7v/6u6/zbROvy/r7yPfnvlvqTlg/Mw6PX3+JclF/6ysqtvtrnq/GVu9rV3Gmr+ju0/v7j/K1KvsgKz9N/NmPkfJOsctu6verSRUWqKqelmmbse3rqpzq+qR86gjMh2AfThTF7rN3L3pYUn+pKpOy3QhsMMXR85p9KOSvLGm05nWS+b/KsnDqurUTBvG/5vn/+dMO+KTauqOt3Irxp9P8mvz674zybd198czdWV+//z7fRvU/XuZuu6+I9NOecWvJ/mxmk7zOjnzaXjd/bUkb8l0qsqFGyx3We6U6T3/eFWdMv/85EYzdPcnM91d4i2ZLth2cne/btU07810IHVqpq73J66zuMMy/Tfv5CSfWRj+9CT7VNX757b7sbnL8qOSvGYetnKg9uok16uqMzL9t+pDWUN3n5qp7T6Y6Y+hd8zDv5ape/pz5uUen7kHU3efnOnUnq06MN+u21UyffG9OtNpaq/u7pNWT9Dd/5JpXb9r/uy/KtNF/87IdCHSt87rfPWX7V5JXjLP874kz+5L3wnpsZm+5E7L9Bn+9V289z1pq7erZDon/z3zdI9e5z94T8t0ivBp8/bztHn4C+b5T5vbZ/VdcG6c5IR53/mSJJf47//8Wo/ItG2fnuk6W8/boNY9YptuSw9M8v55XX53prAmmQ7EfrWqPpDpmnJ/Pe+r7p/kj+Z2OSUX333qUt9dm3gPG+7/Frw+yc/On+OVA8h/mOt62WZe55uw1dvSn1TV6TXdxnvlYubJGvu/nk63fUqSf5nb4fhMFyRd77trlzaxr0zmC/PP6+bq87B/yHQK5Qc2+1qXxU7ZptZrm13Us17br1hru1j08kwXB3/FOst/fqZ97D/M729PHQ9ux23qBZkuuv3eefjfZONerNdK8oa5Lf8tyW+sMc1RufR2sVLr5zPdBOFm3f2eNeY9LcmFVXVqVT1+nmdLjwe36ba2usafrapzM512+8aqWiscPTjJqVX1vkzfPX+xwT7ysCTfP9d5RC75z+dFv5bkoKo6rarOzNSDejvY6m3tEqrqj+f2+Zb5M3TYGpM9bv7b67QkX0/yTxv8zbzh52iudaPjlh1t5YJusC3U1CPjvUkesMnu0exBVXWjTNeaOWDu7UaSmk7tPKi7H7PVtXBpNZ3a+YbuftVW18K4qrp5pvb77q2uZT1Vdf9Mdwv8+a2uZU/bCfu/qvrLTNfl+butroXNcTy4vTkeBBbpqcS2UVUHZrpi/5scQGw/VfULmXqd/a4DCIBJVT0n03+Nn7aradnz5p6/35Pde3Fdlsjx4PbmeBBYTU8lAAAAAIbpqQQAAADAMKESAAAAAMOESgAAAAAMEyoBAAAAMEyoBAAAAMCw/x85wNsjSrYzXAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now print graph comparing the two cross validation, increase bar chart size to include all bar \n",
    "# plot and their name (2b)\n",
    "plt.figure(figsize=(20, 10)) \n",
    "plt.bar(['10 fold accuracy', '2 fold accuracy'],[total_accuracy_10/10,total_accuracy_2/2], width=0.5)\n",
    "plt.bar(['10 fold precision', '2 fold precision'],[total_precision_10/10,total_precision_2/2], width=0.5)\n",
    "plt.bar(['10 fold specificity', '2 fold specificity'],[total_specificity_10/10,total_specificity_2/2], width=0.5)\n",
    "plt.bar(['10 fold sensitivity', '2 fold sensitivity'],[total_sensitivity_10/10,total_sensitivity_2/2], width=0.5)\n",
    "plt.bar(['10 fold f1 score', '2 fold f1 score'],[total_fscore_10/10,total_fscore_2/2], width=0.5)\n",
    "# limit the y axis from 0.8 to 1 to better show the data\n",
    "plt.title('10-fold vs 2-fold cross validation (2b)')\n",
    "plt.ylabel('probability')\n",
    "plt.ylim([0.65, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "2a:\n",
    "\n",
    "KDE's accuracies are all lower than gaussian when 𝜎 is increased from 3 to 15, using the same training set. When 𝜎 is 3, KDE obtains the highest accuracy of 85% compared to 86% for Gaussian, and data of KDE (𝜎=3) is used to compare with Gaussian. The precision of KDE is slightly lower than Gaussian's precision. KDE's recall/sensitivity is much lower than Gaussian, meaning that it predicts a lower proportion of true positives using the same training data (<=50K), suggesting that KDE requires more data for positive prediction to obtain a more accurate positive prediction. \n",
    "\n",
    "Numeric data in this dataset are age, number of years of education and working hour, data from these three attributes are presumedly from the population. With a sample size of 1000 (900 training) and central limit theorem, the numeric attribute are expected to follow a Gaussian distribution. \n",
    "\n",
    "When evaluating a model, we prefer the model that is high in both precision and recall/sensitivity, and f1 score is used to determine which of the model is better. Since Gaussian naïve bayes obtains higher f1 score and accuracy (higher than all accepted KDE standard deviation range) while the data is also follow Gaussian distribution, then Gaussian naïve bayes would be the better model for class prediction for this dataset. \n",
    "\n",
    "2b:\n",
    "\n",
    "The increased m in m-fold cross validation increases partitions of the data, training size is also increased. With more training data, the model can obtain more information about the dataset, covering more potential observable instance in the test data from the partitions, hence expected to obtain more accurate prediction. \n",
    "However, different scenario is observed in 10- and 2-fold cross validation for this dataset. 10-fold CV's average accuracy is 0.815, where 2-fold CV's accuracy is 0.821. \n",
    "\n",
    "A reason for this unusual data is that the dataset we used is unbalanced, the model will be biased towards the majority class(<=50k) since prior of majority class is much higher. We could see that from the graph, where sensitivity is much higher than specificity. \n",
    "\n",
    "10-fold’s precision is higher than 2-fold, and 2-fold predicts a higher proportion of True positive as it has a higher sensitivity. This suggest that 10-fold is more accurate when predicting positive (<=50k), even though proportion of TP predicted is less than 2-fold. 10-fold CV’s specificity is also higher than 2-fold, meaning that it is more accurate in predicting true negative (>50k). From this we can conclude that, even though 2-fold CV’s accuracy is higher than 10-fold, its accuracy is based on the biased model. 10-fold CV is also influenced by the biased prior, however it is a more reliable model since it studies more training data (900 compare to 500). Thus, as m in m-fold cross validation increases, accuracy obtained by the model may reduce, but the model becomes more reliable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 [4 marks]\n",
    "In `train()`, you are asked to treat the missing value of nominal attributes as a new category. There is another option (as suggested in Thu lecture in week 2): <u>ignoring the missing values</u>. \n",
    "Compare the two methods in both large and small datasets. Comment and explain your observations.\n",
    "You can extract the first 50 records to construct a small dataset.Use Gaussian Naive Bayes only for this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide your text answer of 150-200 words in this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 [4 marks]\n",
    "In week 4, we have learned how to obtain information gain (IG) and gain ratio (GR) to choose an attribute to split a node in a decision tree. We will see how to apply them in the Naive Bayes classification.\n",
    "\n",
    "(a) Compute the GR of each attribute $X_i$, relative to the class distribution. In the Na\\\"ive Bayes classifier, remove attributes in the ascending order of GR: first, remove $P(X_i|c_j)$ such that $X_i$ has the least GR; second, remove $P(X_{i'}|c_j)$ such that $X_{i'}$ has the second least GR,......, until there is only one $X_{i*}$ with the largest GR remaining in the maximand $P(c_j) P(X_{i^*} | c_j)$. Observe the <u>change of the accuracy for both Gaussian and KDE</u> (Choose bandwidth $\\sigma=10$ for KDE).\n",
    "\n",
    "(b) Compute the IG between each pair of attributes. Describe and explain your observations. Choose an attribute and implement an estimator to predict the value of `education num`. Explain why you choose this attribute. Enumerate two other examples that an attribute can be used to estimate the other and explain the reason.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write additional code here, if necessary (you may insert additional code cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide your text answer to **Question 4.a** of 100-150 words in this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide your text answer to **Question 4.b** of 150-200 words in this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Authorship Declaration</b>:\n",
    "\n",
    "   (1) I certify that the program contained in this submission is completely\n",
    "   my own individual work, except where explicitly noted by comments that\n",
    "   provide details otherwise.  I understand that work that has been developed\n",
    "   by another student, or by me in collaboration with other students,\n",
    "   or by non-students as a result of request, solicitation, or payment,\n",
    "   may not be submitted for assessment in this subject.  I understand that\n",
    "   submitting for assessment work developed by or in collaboration with\n",
    "   other students or non-students constitutes Academic Misconduct, and\n",
    "   may be penalized by mark deductions, or by other penalties determined\n",
    "   via the University of Melbourne Academic Honesty Policy, as described\n",
    "   at https://academicintegrity.unimelb.edu.au.\n",
    "\n",
    "   (2) I also certify that I have not provided a copy of this work in either\n",
    "   softcopy or hardcopy or any other form to any other student, and nor will\n",
    "   I do so until after the marks are released. I understand that providing\n",
    "   my work to other students, regardless of my intention or any undertakings\n",
    "   made to me by that other student, is also Academic Misconduct.\n",
    "\n",
    "   (3) I further understand that providing a copy of the assignment\n",
    "   specification to any form of code authoring or assignment tutoring\n",
    "   service, or drawing the attention of others to such services and code\n",
    "   that may have been made available via such a service, may be regarded\n",
    "   as Student General Misconduct (interfering with the teaching activities\n",
    "   of the University and/or inciting others to commit Academic Misconduct).\n",
    "   I understand that an allegation of Student General Misconduct may arise\n",
    "   regardless of whether or not I personally make use of such solutions\n",
    "   or sought benefit from such actions.\n",
    "\n",
    "   <b>Signed by</b>: Aobo Li, Student ID:1172339\n",
    "   \n",
    "   <b>Dated</b>: 2022.4.7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
